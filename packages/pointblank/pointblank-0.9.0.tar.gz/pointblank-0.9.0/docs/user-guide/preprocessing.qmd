---
title: Transforming the Target Table
jupyter: python3
html-table-processing: none
---

```{python}
#| echo: false
#| output: false
import pointblank as pb
pb.config(report_incl_footer=False)
```

While the validation methods available can do a lot for you, there's likewise a lot of things you
*can't* easily do with them. What if you wanted to validate:

- string lengths in a column are less than 10 characters
- the median of values in a column is less than the median of values in another column
- there are at least three instances of every categorical value in a column

These are more complicated types of validations, yet checks of this type are very commonplace. We
don't need to have a very large library of validation methods to tackle each an every case; the
number of combinations indeed seems exceedingly large. Instead, let's transform the table we are
validating through a preprocessing step and expose the key values. In conjunction with that sort of
table transformation, we then can use the existing library of validation methods.

Central to this approach is the idea of composability. Pointblank makes it easy to safely transform
the target table for a given validation via the `pre=` argument. Any computed columns are available
for the (short) lifetime of the validation step during interrogation. This sort of composibility
means: (1) we can validate on different forms of the initial dataset (e.g., validating on
aggregate forms, validating on calculated columns, etc.), (2) there's no need to start an entirely
new validation process for each transformed version of the data (i.e., one tabular report could be
produced instead of several).

Now, through a series of examples, let's look at the process of performing the validations mentioned
above. We'll use the `small_table` dataset for all of the examples. Here it is in its entirety:

```{python}
# | echo: false
pb.preview(pb.load_dataset(dataset="small_table"), n_head=20, n_tail=20)
```

## The Basics of Preprocessing the Target Table

In getting to grips with the basics, we'll try to validate that string lengths in the `b` column are
less than 10 characters. We can't directly use the
[`col_vals_lt()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_lt.html)
validation method with that column because it is meant to be used with a column of numeric values.
Let's just give that method what it needs and create a column with string lengths! The target table
is a Polars DataFrame so we'll provide a lambda function that uses the Polars API to add in that
numeric column:

```{python}
import polars as pl

(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        tbl_name="small_table",
        label="String lengths"
    )
    .col_vals_lt(
        columns="string_lengths",  # the generated column through `pre=`
        value=10,                  # the string length value to be less than
        pre=lambda df: df.with_columns(string_lengths=pl.col("b").str.len_chars())
    )
    .interrogate()
)
```

The validation was successfully constructed and we can see from the validation report table that all
strings in `b` had lengths less than 10 characters. Also note that the icon under the `TBL` column
is no longer a rightward-facing arrow, but one that is indicative of a transformation taking place.

Now look at the code itself. Notice that we're not directly testing the `b` column. Instead the test
is of the `string_lengths` column generated by the lambda provided to `pre=`. We used Polars to do
the transformation work here (via its `with_columns()` method) and that's the piece that generates
numerical values of string lengths in the computed column.

That transformation occurs only during interrogation and only for that validation step. Any prior or
subsequent steps would normally use the as-provided `small_table`. Having the possibility for
data transformation being isolated at the step level means that you don't have to generate separate
validation plans for each form of the data, you're free to fluidly transform the target table as
necessary for perform validations on different representations of the data.

Aside from using a lambda, you can pass in a custom function. Just make sure not to evaluate it at
the `pre=` parameter (everything is stored lazily until interrogation time). Here's an example of
that approach:

```{python}
def add_string_lengths(df):
    return df.with_columns(string_lengths=pl.col("b").str.len_chars())

(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        tbl_name="small_table",
        label="String lengths"
    )
    .col_vals_lt(columns=pb.last_n(1), value=10, pre=add_string_lengths)
    .interrogate()
)
```

The column-generating logic was placed in the `add_string_lengths()` function, which is then passed
to `pre=`. We also know that the column generated will be the final one in the column series, so the
`last_n()` column selector obviates the need to provide `"string_lengths"` here (you'll still find
the target column name echoed on the validation report table).

## Using Narwhals to Preprocess Many Types of DataFrames

In this previous example we used a Polars table (the `load_dataset()` returns a Polars DataFrame by
default). You might have a situation where where you perform data validation variously on Pandas and
Polars DataFrames. This is where Narwhals becomes handy.

Let's obtain `small_table` as a Pandas DataFrame. We'll construct a validation step to verify that
the median of column `c` is greater than the median in column `a`.

```{python}
import narwhals as nw

(
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="pandas"),
        tbl_name="small_table",
        label="Median comparison",
    )
    .col_vals_gt(
        columns="c",
        value=pb.col("a"),
        pre=lambda df: nw.from_native(df).select(nw.median("c"), nw.median("a"))
    )
    .interrogate()
)
```

There's a bit to unpack here so let's look at at the lambda function first. Narwhals can translate
a Pandas DataFrame to a Narwhals DataFrame with its `from_native()` function. After that initiating
step, you're free to use the Narwhals API (which is modeled on a subset of the Polars API) to do the
necessary data transformation. In this case, we are getting the medians of the `c` and `a` columns
and ending up with a one-row, two-column table.

The goal is to check that the median value of `c` is greater than the corresponding median of
column `a`, so we set up `columns=` and `value=` parameters in that way within the
[`col_vals_gt()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_gt.html)
validation method call.

## Swapping in a Totally Different DataFrame

Let's now try to prepare the final validation scenario, checking that there are at least three
instances of every categorical value in column `f` (which contains string values in the set of
`"low"`, `"mid"`, and `"high"`). This time, we'll prepare the transformed table (transformed by
Polars expressions) outside of the Pointblank code.

```{python}
data_original = pb.load_dataset("small_table")
data_transformed = data_original.group_by("f").len(name="n")

data_transformed
```

Then, we'll plug in the `data_transformed` DataFrame with a lambda expression in `pre=`:

```{python}
data_original = pb.load_dataset("small_table")
data_transformed = data_original.group_by("f").len(name="n")

validation = (
    pb.Validate(
        data=pb.load_dataset(dataset="small_table", tbl_type="polars"),
        tbl_name="small_table",
        label="Category counts",
    )
    .col_vals_ge(columns="n", value=3, pre=lambda x: data_transformed)
    .interrogate()
)

validation
```

We can see from the validation report table that there are three test units. This corresponds to a
row for each of the categorical value counts. From the report, we find that two of the three test
units are passing test units (turns out there are only two instances of `"mid"` in column `f`).
