import os
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor

from google import genai
from google.genai import types
from ..._core import Core, ImageInterpreter
from ..._util import CreatorRole, ChatCompletionConfig, MessageBlock, TokenUsage
from .base import GeminiCore

logger = logging.getLogger(__name__)


class I2T_GMN_Core(Core, GeminiCore, ImageInterpreter):
    """
    `I2T_GMN_Core` is abstract base classes `Core`, `GeminiCore` and `ImageInterpreter`.
    It facilitates synchronous and asynchronous communication with Gemini's API.

    Methods:
    - run(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Synchronously run the LLM model with the given query and context.
    - run_async(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Asynchronously run the LLM model with the given query and context.
    - interpret(query: str, context: list[MessageBlock | dict] | None, filepath: str, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Synchronously interpret the given image.
    - interpret_async(query: str, context: list[MessageBlock | dict] | None, filepath: str, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Asynchronously interpret the given image.
    """

    SUPPORTED_IMAGE_FORMATS = (".png", ".jpeg", ".jpg", ".webp")

    def __init__(self, system_prompt: str, config: ChatCompletionConfig):
        Core.__init__(self, system_prompt, config)
        GeminiCore.__init__(self, config.name)
        self.profile = self.build_profile(config.name)

    def custom_config(self, max_output_tokens: int) -> types.GenerateContentConfig:
        """Adapter function.

        Transform custom ChatCompletionConfig -> types.GenerationContentConfig
        """
        config = types.GenerateContentConfig(
            system_instruction=self.system_prompt,
            temperature=self.config.temperature,
            max_output_tokens=max_output_tokens,
        )
        return config

    def run(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict], TokenUsage]:
        """
        Synchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            list[MessageBlock | dict]: The list of messages generated by the LLM model.
            TokenUsage: The recorded token usage.

        Notes:
        * Single-Turn Execution.
        """
        filepath: str | None = kwargs.get("filepath", None)
        if filepath:
            ext = os.path.splitext(filepath)[-1]
            if ext not in I2T_GMN_Core.SUPPORTED_IMAGE_FORMATS:
                raise ValueError(f"Unsupported image type: {ext}")

        msgs: list[types.Content] = self.preprocessing(query, context, filepath)

        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        prompt_token_count = self.calculate_token_count(
            self.model_name,
            self.system_prompt,
            msgs,
            imgs=None if filepath is None else [filepath],
        )
        max_output_tokens = min(
            MAX_OUTPUT_TOKENS,
            self.context_length - prompt_token_count,
        )

        config = self.custom_config(max_output_tokens)
        try:
            client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
            response = client.models.generate_content(
                model=self.model_name,
                contents=msgs,  # type: ignore
                config=config,
            )

            token_usage = self.update_usage(response.usage_metadata, token_usage=None)

            response_text = self.get_response_text(response)
            if response_text:
                msgs.append(
                    types.Content(
                        role=CreatorRole.MODEL.value,
                        parts=[types.Part.from_text(text=response_text)],
                    )
                )

            output = self.postprocessing(msgs[-1:])
            return output, token_usage
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise

    @staticmethod
    async def acall(
        model_name: str, config: types.GenerateContentConfig, msgs: list[types.Content]
    ):
        """Use this to make the `generate_content` method asynchronous."""
        client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
        with ThreadPoolExecutor() as executor:
            future = executor.submit(
                client.models.generate_content,
                model=model_name,
                contents=msgs,  # type: ignore
                config=config,
            )
            response = await asyncio.wrap_future(future)  # Makes the future awaitable
            return response

    async def run_async(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict], TokenUsage]:
        """
        Asynchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            list[MessageBlock | dict]: The list of messages generated by the LLM model.
            TokenUsage: The recorded token usage.

        Notes:
        * Single-Turn Execution.
        """
        filepath: str | None = kwargs.get("filepath", None)
        if filepath:
            ext = os.path.splitext(filepath)[-1]
            if ext not in I2T_GMN_Core.SUPPORTED_IMAGE_FORMATS:
                raise ValueError(f"Unsupported image type: {ext}")

        msgs: list[types.Content] = self.preprocessing(query, context, filepath)

        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        prompt_token_count = self.calculate_token_count(
            self.model_name,
            self.system_prompt,
            msgs,
            imgs=None if filepath is None else [filepath],
        )
        max_output_tokens = min(
            MAX_OUTPUT_TOKENS,
            self.context_length - prompt_token_count,
        )

        config = self.custom_config(max_output_tokens)
        try:
            response = await self.acall(self.model_name, config, msgs)

            token_usage = self.update_usage(response.usage_metadata, token_usage=None)

            response_text = self.get_response_text(response)
            if response_text:
                msgs.append(
                    types.Content(
                        role=CreatorRole.MODEL.value,
                        parts=[types.Part.from_text(text=response_text)],
                    )
                )

            output = self.postprocessing(msgs[-1:])
            return output, token_usage
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise

    def interpret(
        self,
        query: str,
        context: list[MessageBlock | dict] | None,
        filepath: str,
        **kwargs,
    ) -> tuple[list[MessageBlock | dict], TokenUsage]:
        ext = os.path.splitext(filepath)[-1]
        if ext not in I2T_GMN_Core.SUPPORTED_IMAGE_FORMATS:
            raise ValueError(f"Unsupported image type: {ext}")

        return self.run(query=query, context=context, filepath=filepath, **kwargs)

    async def interpret_async(
        self,
        query: str,
        context: list[MessageBlock | dict] | None,
        filepath: str,
        **kwargs,
    ) -> tuple[list[MessageBlock | dict], TokenUsage]:
        ext = os.path.splitext(filepath)[-1]
        if ext not in I2T_GMN_Core.SUPPORTED_IMAGE_FORMATS:
            raise ValueError(f"Unsupported image type: {ext}")

        return await self.run_async(
            query=query, context=context, filepath=filepath, **kwargs
        )
