import os
import logging
from typing import Any
import openai
from ..._core import Core
from ..._util import CreatorRole, ChatCompletionConfig, MessageBlock, TokenUsage
from .base import OpenAICore

logger = logging.getLogger(__name__)


class O1Beta_OAI_Core(Core, OpenAICore):
    """
    `O1Beta_OAI_Core` is a concrete implementation of abstract base classes `Core`.
    `O1Beta_OAI_Core` is also a child class of `OpenAICore`.

    It facilitates synchronous and asynchronous communication with OpenAI's API.

    This is specially implemented to support `o1-preview` and `o1-mini`.
    Things are expected to change.

    Methods:
    - run(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Synchronously run the LLM model with the given query and context.
    - run_async(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Asynchronously run the LLM model with the given query and context.


    **Notes:**
    - The caller is responsible for memory management, output parsing and error handling.
    - If model is not available under OpenAI's listing, raise ValueError.
    - `context_length` is configurable.
    - `max_output_tokens` is configurable.
    """

    SUPPORTED_MODELS = ("o1-preview", "o1-mini")

    def __init__(
        self,
        system_prompt: str,
        config: ChatCompletionConfig,
    ):
        if config.name not in self.SUPPORTED_MODELS:
            raise ValueError(
                f"{config.name} is not supported. Supported models: {self.SUPPORTED_MODELS}"
            )
        if config.temperature != 1.0:
            raise ValueError("temperature has to be 1.0")

        Core.__init__(self, system_prompt, config)
        OpenAICore.__init__(self, config.name)
        self.profile = self.build_profile(config.name)

    async def run_async(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Asynchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            list[MessageBlock | dict]: The list of messages generated by the LLM model.
            TokenUsage: The recorded token usage.

        Notes:
        * No system prompt!
        * max_tokens -> max_completion_tokens
        * temperature has to be 1.0
        * frequency_penalty not supported
        * reasoning_effort is only available to `o1`
        """
        # MessageBlock(role=CreatorRole.USER.value, content=self.system_prompt)
        msgs: list[MessageBlock | dict] = []

        if context:
            msgs.extend(context)
        msgs.append(MessageBlock(role=CreatorRole.USER.value, content=query))

        # Determine the maximum number of tokens allowed for the response
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        prompt_token_count = self.calculate_token_count(msgs, None, None, None)
        max_output_tokens = min(
            MAX_OUTPUT_TOKENS,
            self.context_length - prompt_token_count,
        )

        if max_output_tokens <= 0:
            logger.warning("Prompt token count: %d", prompt_token_count)
            raise ValueError("max_output_tokens <= 0")

        try:
            client = openai.AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])
            response = await client.chat.completions.create(
                model=self.model_name,
                messages=msgs,  # type: ignore
                # frequency_penalty=0.5,
                max_completion_tokens=max_output_tokens,
                temperature=self.config.temperature,
                n=self.config.return_n,
            )

            choice = response.choices[0]
            _content = getattr(choice.message, "content", None)

            token_usage = self.update_usage(response.usage)

            if _content:
                return [
                    {"role": CreatorRole.ASSISTANT.value, "content": _content}
                ], token_usage

            failed_reason = choice.finish_reason
            raise RuntimeError(failed_reason)
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise

    def run(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Synchronously generate text based on the given query and context.

        Args:
            query (str): The query to generate text for.
            context (list): A list of context messages or dictionaries.
            **kwargs: Additional keyword arguments.

        Returns:
            list[MessageBlock | dict]: The list of messages generated by the LLM model.
            TokenUsage: The recorded token usage.

        Notes:
        * No system prompt!
        * max_tokens -> max_completion_tokens
        * temperature has to be 1.0
        * frequency_penalty not supported
        * reasoning_effort is only available to `o1`
        """
        # MessageBlock(role=CreatorRole.USER.value, content=self.system_prompt)
        msgs: list[MessageBlock | dict] = []

        if context:
            msgs.extend(context)
        msgs.append(MessageBlock(role=CreatorRole.USER.value, content=query))

        # Determine the maximum number of tokens allowed for the response
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        prompt_token_count = self.calculate_token_count(msgs, None, None, None)
        max_output_tokens = min(
            MAX_OUTPUT_TOKENS,
            self.context_length - prompt_token_count,
        )

        if max_output_tokens <= 0:
            logger.warning("Prompt token count: %d", prompt_token_count)
            raise ValueError("max_output_tokens <= 0")

        try:
            client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])

            response = client.chat.completions.create(
                model=self.model_name,
                messages=msgs,  # type: ignore
                # frequency_penalty=0.5,
                max_completion_tokens=max_output_tokens,
                temperature=self.config.temperature,
                n=self.config.return_n,
            )

            choice = response.choices[0]
            _content = getattr(choice.message, "content", None)

            token_usage = self.update_usage(response.usage)

            if _content:
                return [
                    {"role": CreatorRole.ASSISTANT.value, "content": _content}
                ], token_usage

            failed_reason = choice.finish_reason
            raise RuntimeError(failed_reason)
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise
