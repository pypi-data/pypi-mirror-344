import os
import json
import logging
from typing import Any, Optional, Type, TypeVar
import base64

# External Packages
import openai
from pydantic import BaseModel

# Self Defined Packages
from ..._util import (
    CreatorRole,
    ChatCompletionConfig,
    MessageBlock,
    ResponseMode,
    TokenUsage,
)
from ..._core import Core, ImageInterpreter
from .base import OpenAICore

T = TypeVar("T", bound=BaseModel)
logger = logging.getLogger(__name__)


class OAI_StructuredOutput_Core(Core, OpenAICore, ImageInterpreter):
    """
    `OAI_StructuredOutput_Core` is the child class of OpenAICore and a concrete implementation of abstract base classes `Core` and `ImageInterpreter`.

    It is particularly suited for chaining operations where structured outputs are required.

    Methods:
    - run(query: str, context: list[MessageBlock | dict[str, Any]] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:

        Synchronously run the LLM model with the given query and context.

    - run_async(query: str, context: list[MessageBlock | dict[str, Any]] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:

        Asynchronously run the LLM model with the given query and context.

    - interpret(query: str, context: list[MessageBlock | dict[str, Any]] | None, filepath: str, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:

        Synchronously interpret the given image.

    - interpret_async(query: str, context: list[MessageBlock | dict[str, Any]] | None, filepath: str, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:

        Asynchronously interpret the given image.

    Notes:
    - The caller is responsible for memory management, output parsing and error handling.
    - The caller is responsible for choosing models that support `Vision` and `Structured Ouput`.
        - Supported image format: .png, .jpeg, .jpg, .gif, .webp
        - Supported mode: structured_output, json_object
    - Configurable Parameters: `context_length` and `max_output_tokens`.
    - Best suited for chaining operations where structured data flow is essential.
    - https://platform.openai.com/docs/guides/structured-outputs
    """

    SUPPORTED_IMAGE_FORMATS = (".png", ".jpeg", ".jpg", ".gif", ".webp")

    def __init__(
        self,
        system_prompt: str,
        config: ChatCompletionConfig,
    ):
        Core.__init__(self, system_prompt, config)
        OpenAICore.__init__(self, config.name)
        self.profile = self.build_profile(model_name=config.name)

    def validate(
        self, response_mode: Optional[ResponseMode], response_format: Optional[Type[T]]
    ) -> None:
        if response_mode:
            if not isinstance(response_mode, ResponseMode):
                raise TypeError(
                    f"Expect mode to be an instance of 'ResponseMode', but got '{type(response_mode).__name__}'."
                )
            if response_mode is response_mode.SO:
                if response_format is None:
                    raise TypeError(
                        "Expect format to be a subclass of 'BaseModel', but got 'NoneType'."
                    )
                if not issubclass(response_format, BaseModel):
                    raise TypeError(
                        f"Expect format to be a subclass of 'BaseModel', but got '{type(response_format).__name__}'."
                    )

    def run(
        self, query: str, context: list[MessageBlock | dict[str, Any]] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Synchronously generate text based on the given query and context.

        Support image interpretation and structured output.

        Args:
            query (str): The query to generate text for.
            context (list): A list of context messages or dictionaries.
            **kwargs: Additional keyword arguments:
                * `filepath` (str | None): Path to the image file.
                * `mode` (ResponseMode | None): Ouput mode.
                * `format` (BaseModel | None): Output structure.

        Returns:
            list[MessageBlock|dict]:
                * The list of messages generated by the LLM model.
                    Only 1 element in the list, it's content can be decoded by `json.loads()`.
            TokenUsage: The recorded token usage.

        Raises:
            TypeError:
                * `mode` is not type `ResponseMode`.
                * `format` is not a subclass of `BaseModel` when `mode` is `ResponseMode.SO`.
            FileNotFoundError:
                * If `filepath` is found.
            ValueError:
                * If max_output_tokens <= 0. Theres no capacity left for text-generation.
                * If `filepath` is not a supported image format.
        """
        response_mode: Optional[ResponseMode] = kwargs.get("mode", ResponseMode.DEFAULT)
        response_format: Optional[Type[T]] = kwargs.get("format")  # type: ignore
        self.validate(response_mode, response_format)  # Raise an exception if invalid

        msgs: list[MessageBlock | dict[str, Any]] = [
            {"role": CreatorRole.SYSTEM.value, "content": self.system_prompt}
        ]

        if context:
            msgs.extend(context)

        filepath: str | None = kwargs.get("filepath", None)
        if filepath:
            # detail hardcode as "high"
            resized, newpath = self.resize(filepath, "high")
            if resized and newpath:
                img_url = self.get_image_url(newpath)
                os.remove(newpath)
            else:
                img_url = self.get_image_url(filepath)

            msgs.append(
                {
                    "role": CreatorRole.USER.value,
                    "content": [
                        {"type": "text", "text": query},
                        {
                            "type": "image_url",
                            "image_url": {"url": img_url, "detail": "high"},
                        },
                    ],  # type: ignore
                }
            )
        else:
            msgs.append(MessageBlock(role=CreatorRole.USER.value, content=query))

        # Determine the maximum number of tokens allowed for the response
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        prompt_token_count = self.calculate_token_count(
            msgs,
            None,
            images=[filepath] if filepath else None,
            image_detail="high" if filepath else None,
        )
        max_output_tokens = min(
            MAX_OUTPUT_TOKENS,
            self.context_length - prompt_token_count,
        )

        if max_output_tokens <= 0:
            logger.warning("Prompt token count: %d", prompt_token_count)
            raise ValueError("max_output_tokens <= 0")

        try:
            client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])
            if response_mode is ResponseMode.SO and response_format:
                response = client.beta.chat.completions.parse(
                    model=self.model_name,
                    messages=msgs,  # type: ignore
                    frequency_penalty=0.5,
                    max_tokens=max_output_tokens,
                    temperature=self.config.temperature,
                    n=self.config.return_n,
                    response_format=response_format,  # type: ignore
                )
            elif response_mode is ResponseMode.JSON:
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=msgs,  # type: ignore
                    frequency_penalty=0.5,
                    max_tokens=max_output_tokens,
                    temperature=self.config.temperature,
                    n=self.config.return_n,
                    response_format={"type": "json_object"},  # type: ignore
                )
            else:
                # response_mode is ResponseMode.DEFAULT
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=msgs,  # type: ignore
                    frequency_penalty=0.5,
                    max_tokens=max_output_tokens,
                    temperature=self.config.temperature,
                    n=self.config.return_n,
                )

            choice = response.choices[0]
            _content = getattr(choice.message, "content", "Not Available")

            token_usage = self.update_usage(response.usage)
            logger.debug("Usage: %s", token_usage)

            if _content:
                if response_mode is not ResponseMode.DEFAULT:
                    try:
                        # Validate JSON format
                        _ = json.loads(_content)
                        content = _content
                    except json.JSONDecodeError as decode_error:
                        e = {"error": str(decode_error), "text": _content}
                        content = json.dumps(e)
                else:
                    content = _content
                return [
                    {"role": CreatorRole.ASSISTANT.value, "content": content}
                ], token_usage
            raise RuntimeError(f"Content not available. Reason: {choice.finish_reason}")
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise

    async def run_async(
        self, query: str, context: list[MessageBlock | dict[str, Any]] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Asynchronously generate text based on the given query and context.

        Support image interpretation and structured output.

        Args:
            query (str): The query to generate text for.
            context (list): A list of context messages or dictionaries.
            **kwargs: Additional keyword arguments:
                * `filepath` (str | None): Path to the image file.
                * `mode` (ResponseMode | None): Ouput mode.
                * `format` (BaseModel | None): Output structure.

        Returns:
            list[MessageBlock|dict]:
                * The list of messages generated by the LLM model.
                    Only 1 element in the list, it's content can be decoded by `json.loads()`.
            TokenUsage: The recorded token usage.

        Raises:
            TypeError:
                * `mode` is not type `ResponseMode`.
                * `format` is not a subclass of `BaseModel` when `mode` is `ResponseMode.SO`.
            FileNotFoundError:
                * If `filepath` is found.
            ValueError:
                * If max_output_tokens <= 0. Theres no capacity left for text-generation.
                * If `filepath` is not a supported image format.
        """
        response_mode: Optional[ResponseMode] = kwargs.get("mode", ResponseMode.DEFAULT)
        response_format: Optional[Type[T]] = kwargs.get("format")  # type: ignore
        self.validate(response_mode, response_format)  # Raise an exception if invalid

        msgs: list[MessageBlock | dict[str, Any]] = [
            {"role": CreatorRole.SYSTEM.value, "content": self.system_prompt}
        ]

        if context:
            msgs.extend(context)

        filepath: str | None = kwargs.get("filepath", None)
        if filepath:
            # detail hardcode as "high"
            resized, newpath = self.resize(filepath, "high")
            if resized and newpath:
                img_url = self.get_image_url(newpath)
                os.remove(newpath)
            else:
                img_url = self.get_image_url(filepath)

            msgs.append(
                {
                    "role": CreatorRole.USER.value,
                    "content": [
                        {"type": "text", "text": query},
                        {
                            "type": "image_url",
                            "image_url": {"url": img_url, "detail": "high"},
                        },
                    ],  # type: ignore
                }
            )
        else:
            msgs.append(MessageBlock(role=CreatorRole.USER.value, content=query))

        # Determine the maximum number of tokens allowed for the response
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        prompt_token_count = self.calculate_token_count(
            msgs,
            None,
            images=[filepath] if filepath else None,
            image_detail="high" if filepath else None,
        )
        max_output_tokens = min(
            MAX_OUTPUT_TOKENS,
            self.context_length - prompt_token_count,
        )

        if max_output_tokens <= 0:
            logger.warning("Prompt token count: %d", prompt_token_count)
            raise ValueError("max_output_tokens <= 0")
        try:
            client = openai.AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])
            if response_mode is ResponseMode.SO and response_format:
                response = await client.beta.chat.completions.parse(
                    model=self.model_name,
                    messages=msgs,  # type: ignore
                    frequency_penalty=0.5,
                    max_tokens=max_output_tokens,
                    temperature=self.config.temperature,
                    n=self.config.return_n,
                    response_format=response_format,  # type: ignore
                )
            elif response_mode is ResponseMode.JSON:
                response = await client.chat.completions.create(
                    model=self.model_name,
                    messages=msgs,  # type: ignore
                    frequency_penalty=0.5,
                    max_tokens=max_output_tokens,
                    temperature=self.config.temperature,
                    n=self.config.return_n,
                    response_format={"type": "json_object"},  # type: ignore
                )
            else:
                # response_mode is ResponseMode.DEFAULT
                response = await client.chat.completions.create(
                    model=self.model_name,
                    messages=msgs,  # type: ignore
                    frequency_penalty=0.5,
                    max_tokens=max_output_tokens,
                    temperature=self.config.temperature,
                    n=self.config.return_n,
                )

            choice = response.choices[0]
            _content = getattr(choice.message, "content", "Not Available")

            token_usage = self.update_usage(response.usage)
            logger.debug("Usage: %s", token_usage)

            if _content:
                if response_mode is not ResponseMode.DEFAULT:
                    try:
                        _ = json.loads(_content)
                        content = _content
                    except json.JSONDecodeError as decode_error:
                        e = {"error": str(decode_error), "text": _content}
                        content = json.dumps(e)
                else:
                    content = _content
                return [
                    {"role": CreatorRole.ASSISTANT.value, "content": content}
                ], token_usage

            raise RuntimeError(f"Content not available. Reason: {choice.finish_reason}")
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise

    @staticmethod
    def get_image_url(filepath: str) -> str:
        ext = os.path.splitext(filepath)[-1]
        if ext not in OAI_StructuredOutput_Core.SUPPORTED_IMAGE_FORMATS:
            raise ValueError(f"Unsupported image type: {ext}")
        ext = ext[1:] if ext != ".jpg" else "jpeg"
        try:
            with open(filepath, "rb") as f:
                encoded_image = base64.b64encode(f.read()).decode("utf-8")
                return f"data:image/{ext};base64,{encoded_image}"
        except FileNotFoundError as fnfe:
            logger.error("FileNotFoundError: %s", fnfe, exc_info=True, stack_info=True)
            raise
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise

    def interpret(
        self,
        query: str,
        context: list[MessageBlock | dict[str, Any]] | None,
        filepath: str,
        **kwargs,
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        return self.run(query=query, context=context, filepath=filepath, **kwargs)

    async def interpret_async(
        self,
        query: str,
        context: list[MessageBlock | dict[str, Any]] | None,
        filepath: str,
        **kwargs,
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        return await self.run_async(
            query=query, context=context, filepath=filepath, **kwargs
        )
