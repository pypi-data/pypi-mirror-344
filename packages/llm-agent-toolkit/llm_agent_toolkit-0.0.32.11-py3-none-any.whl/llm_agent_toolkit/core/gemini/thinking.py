import os
import logging
import asyncio
from typing import Any
from concurrent.futures import ThreadPoolExecutor

from google import genai
from google.genai import types
from ..._core import Core
from ..._util import CreatorRole, ChatCompletionConfig, MessageBlock, TokenUsage
from .base import GeminiCore

logger = logging.getLogger(__name__)


class Thinking_Core(Core, GeminiCore):
    """
    `Thinking_Core` is abstract base classes `Core` and `GeminiCore`.
    It facilitates synchronous and asynchronous communication with Gemini's API.

    Methods:
    - run(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Synchronously run the LLM model with the given query and context.
    - run_async(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Asynchronously run the LLM model with the given query and context.

    Notes:
    - The *thinking* process is not in the output of generation.
    - https://ai.google.dev/gemini-api/docs/thinking
    - At this point, this is very similar to `T2T_GMN_Core`.
    """

    def __init__(self, system_prompt: str, config: ChatCompletionConfig):
        Core.__init__(self, system_prompt, config)
        GeminiCore.__init__(self, config.name)
        self.profile = self.build_profile(config.name)

    def custom_config(self, max_output_tokens: int) -> types.GenerateContentConfig:
        """Adapter function.

        Transform custom ChatCompletionConfig -> types.GenerationContentConfig
        """
        config = types.GenerateContentConfig(
            system_instruction=self.system_prompt,
            temperature=self.config.temperature,
            max_output_tokens=max_output_tokens,
        )
        # thinking_config=types.ThinkingConfig(include_thoughts=True)
        # thinkingConfig not yet supported (google-genai==1.0.0)
        return config

    def run(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Synchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            list[MessageBlock | dict]: The list of messages generated by the LLM model.
            TokenUsage: The recorded token usage.

        Notes:
        * Single-Turn Execution.
        """
        msgs: list[types.Content] = self.preprocessing(query, context)
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        prompt_token_count = self.calculate_token_count(
            self.model_name, self.system_prompt, msgs, imgs=None
        )
        max_output_tokens = min(
            MAX_OUTPUT_TOKENS,
            self.context_length - prompt_token_count,
        )

        config = self.custom_config(max_output_tokens)
        try:
            client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
            response = client.models.generate_content(
                model=self.model_name,
                contents=msgs,  # type: ignore
                config=config,
            )

            token_usage = self.update_usage(response.usage_metadata, token_usage=None)

            response_text = self.get_response_text(response)
            if response_text:
                msgs.append(
                    types.Content(
                        role=CreatorRole.MODEL.value,
                        parts=[types.Part.from_text(text=response_text)],
                    )
                )

            output = self.postprocessing(msgs[-1:])
            return output, token_usage
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise

    @staticmethod
    async def acall(
        model_name: str, config: types.GenerateContentConfig, msgs: list[types.Content]
    ):
        """Use this to make the `generate_content` method asynchronous."""
        client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
        with ThreadPoolExecutor() as executor:
            future = executor.submit(
                client.models.generate_content,
                model=model_name,
                contents=msgs,  # type: ignore
                config=config,
            )
            response = await asyncio.wrap_future(future)  # Makes the future awaitable
            return response

    async def run_async(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Asynchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            list[MessageBlock | dict]: The list of messages generated by the LLM model.
            TokenUsage: The recorded token usage.

        Notes:
        * Single-Turn Execution.
        """
        msgs: list[types.Content] = self.preprocessing(query, context)
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        prompt_token_count = self.calculate_token_count(
            self.model_name, self.system_prompt, msgs, imgs=None
        )
        max_output_tokens = min(
            MAX_OUTPUT_TOKENS,
            self.context_length - prompt_token_count,
        )

        config = self.custom_config(max_output_tokens)
        try:
            response = await self.acall(self.model_name, config, msgs)
            token_usage = self.update_usage(response.usage_metadata, token_usage=None)

            response_text = self.get_response_text(response)
            if response_text:
                msgs.append(
                    types.Content(
                        role=CreatorRole.MODEL.value,
                        parts=[types.Part.from_text(text=response_text)],
                    )
                )

            output = self.postprocessing(msgs[-1:])
            return output, token_usage
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise
