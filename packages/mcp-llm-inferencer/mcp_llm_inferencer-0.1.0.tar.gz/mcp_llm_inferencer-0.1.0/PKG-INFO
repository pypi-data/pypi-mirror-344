Metadata-Version: 2.4
Name: mcp-llm_inferencer
Version: 0.1.0
Summary: Uses Claude or OpenAI API to convert prompt-mapped input into concrete MCP server components such as tools, resource templates, and prompt handlers
Home-page: https://github.com/Sumedh99/mcp_llm_inferencer
Author: Sumedh99
Author-email: Sumedh99@users.noreply.github.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: requires-python
Dynamic: summary

# mcp-llm_inferencer

Uses Claude or OpenAI API to convert prompt-mapped input into concrete MCP server components such as tools, resource templates, and prompt handlers.

## Features

- LLM call engine with retry and fallback logic
- Supports Claude and OpenAI interchangeably
- Streaming support for Claude Desktop
- Tool and resource response validation
- Structured output bundling per component

## Installation

```bash
pip install mcp-llm_inferencer
```

## Usage

```python
from mcp_llm_inferencer import Mcp_llm_inferencer

# Initialize the library
mcp_llm_inferencer_instance = Mcp_llm_inferencer()

# Use the library functions
# Example usage will be added in future versions
```

## License

MIT
