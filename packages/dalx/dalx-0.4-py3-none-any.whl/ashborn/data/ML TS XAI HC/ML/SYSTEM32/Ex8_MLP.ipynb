{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a00ba2-2808-4921-9df4-79a2911bc670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (303, 14)\n",
      "Training shape: (212, 13) | Testing shape: (91, 13)\n",
      "\n",
      "Training with activation='relu' and hidden_layers=(8, 8, 8)\n",
      "\n",
      "Train Results:\n",
      "[[ 71  27]\n",
      " [ 13 101]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.72      0.78        98\n",
      "           1       0.79      0.89      0.83       114\n",
      "\n",
      "    accuracy                           0.81       212\n",
      "   macro avg       0.82      0.81      0.81       212\n",
      "weighted avg       0.82      0.81      0.81       212\n",
      "\n",
      "\n",
      "Test Results:\n",
      "[[34  6]\n",
      " [ 5 46]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86        40\n",
      "           1       0.88      0.90      0.89        51\n",
      "\n",
      "    accuracy                           0.88        91\n",
      "   macro avg       0.88      0.88      0.88        91\n",
      "weighted avg       0.88      0.88      0.88        91\n",
      "\n",
      "\n",
      "Training with activation='identity' and hidden_layers=(8, 8, 8)\n",
      "\n",
      "Train Results:\n",
      "[[ 69  29]\n",
      " [ 14 100]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.70      0.76        98\n",
      "           1       0.78      0.88      0.82       114\n",
      "\n",
      "    accuracy                           0.80       212\n",
      "   macro avg       0.80      0.79      0.79       212\n",
      "weighted avg       0.80      0.80      0.80       212\n",
      "\n",
      "\n",
      "Test Results:\n",
      "[[32  8]\n",
      " [ 4 47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        40\n",
      "           1       0.85      0.92      0.89        51\n",
      "\n",
      "    accuracy                           0.87        91\n",
      "   macro avg       0.87      0.86      0.86        91\n",
      "weighted avg       0.87      0.87      0.87        91\n",
      "\n",
      "\n",
      "Training with activation='tanh' and hidden_layers=(8, 8, 8)\n",
      "\n",
      "Train Results:\n",
      "[[ 74  24]\n",
      " [ 12 102]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.76      0.80        98\n",
      "           1       0.81      0.89      0.85       114\n",
      "\n",
      "    accuracy                           0.83       212\n",
      "   macro avg       0.83      0.82      0.83       212\n",
      "weighted avg       0.83      0.83      0.83       212\n",
      "\n",
      "\n",
      "Test Results:\n",
      "[[35  5]\n",
      " [ 4 47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89        40\n",
      "           1       0.90      0.92      0.91        51\n",
      "\n",
      "    accuracy                           0.90        91\n",
      "   macro avg       0.90      0.90      0.90        91\n",
      "weighted avg       0.90      0.90      0.90        91\n",
      "\n",
      "\n",
      "Training with activation='logistic' and hidden_layers=(8, 8, 8)\n",
      "\n",
      "Train Results:\n",
      "[[  0  98]\n",
      " [  0 114]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        98\n",
      "           1       0.54      1.00      0.70       114\n",
      "\n",
      "    accuracy                           0.54       212\n",
      "   macro avg       0.27      0.50      0.35       212\n",
      "weighted avg       0.29      0.54      0.38       212\n",
      "\n",
      "\n",
      "Test Results:\n",
      "[[ 0 40]\n",
      " [ 0 51]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        40\n",
      "           1       0.56      1.00      0.72        51\n",
      "\n",
      "    accuracy                           0.56        91\n",
      "   macro avg       0.28      0.50      0.36        91\n",
      "weighted avg       0.31      0.56      0.40        91\n",
      "\n",
      "\n",
      "Training with activation='relu' and hidden_layers=(10, 10, 10)\n",
      "\n",
      "Train Results:\n",
      "[[82 16]\n",
      " [15 99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.84        98\n",
      "           1       0.86      0.87      0.86       114\n",
      "\n",
      "    accuracy                           0.85       212\n",
      "   macro avg       0.85      0.85      0.85       212\n",
      "weighted avg       0.85      0.85      0.85       212\n",
      "\n",
      "\n",
      "Test Results:\n",
      "[[35  5]\n",
      " [ 7 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85        40\n",
      "           1       0.90      0.86      0.88        51\n",
      "\n",
      "    accuracy                           0.87        91\n",
      "   macro avg       0.87      0.87      0.87        91\n",
      "weighted avg       0.87      0.87      0.87        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Upload and read dataset\n",
    "df = pd.read_csv('heart.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Normalize predictors\n",
    "target_column = 'target'\n",
    "predictors = [col for col in df.columns if col != target_column]\n",
    "df[predictors] = df[predictors] / df[predictors].max()\n",
    "\n",
    "# Split data\n",
    "X = df[predictors].values\n",
    "y = df[target_column].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\n",
    "print(f\"Training shape: {X_train.shape} | Testing shape: {X_test.shape}\")\n",
    "\n",
    "# Function to train and evaluate MLPClassifier\n",
    "def train_and_evaluate(activation, hidden_layers=(8,8,8), max_iter=500):\n",
    "    print(f\"\\nTraining with activation='{activation}' and hidden_layers={hidden_layers}\")\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layers, activation=activation,\n",
    "                        solver='adam', max_iter=max_iter, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    predict_train = mlp.predict(X_train)\n",
    "    predict_test = mlp.predict(X_test)\n",
    "\n",
    "    print(\"\\nTrain Results:\")\n",
    "    print(confusion_matrix(y_train, predict_train))\n",
    "    print(classification_report(y_train, predict_train, zero_division=0))\n",
    "\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(confusion_matrix(y_test, predict_test))\n",
    "    print(classification_report(y_test, predict_test, zero_division=0))\n",
    "\n",
    "\n",
    "# Try different activation functions\n",
    "for activation in ['relu', 'identity', 'tanh', 'logistic']:\n",
    "    train_and_evaluate(activation)\n",
    "\n",
    "# Try different hidden layer configuration\n",
    "train_and_evaluate('relu', hidden_layers=(10,10,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171896d2-bf99-4d87-9cde-212df4c49cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7012987 , 1.        , 0.66666667, ..., 0.        , 0.25      ,\n",
       "        0.66666667],\n",
       "       [0.67532468, 1.        , 0.        , ..., 1.        , 0.25      ,\n",
       "        0.66666667],\n",
       "       [0.83116883, 1.        , 0.        , ..., 0.5       , 0.25      ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.87012987, 1.        , 0.        , ..., 0.5       , 0.75      ,\n",
       "        0.66666667],\n",
       "       [0.57142857, 1.        , 0.33333333, ..., 1.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.62337662, 1.        , 0.        , ..., 1.        , 0.5       ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd64e2-cca8-4b35-93d8-dbe7b30da280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
