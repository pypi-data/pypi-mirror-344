# pf-reasoning-tool-proj/pf_reasoning_tool/yamls/vision_llm_tool.yaml

$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Tool.schema.json
name: Vision LLM Call # Your custom name
description: "Calls a vision-capable LLM (like GPT-4V) using text and image inputs via a PromptTemplate." # Updated description
type: custom_llm # <<< Correct type for this approach
category: HW_PF_tools # Your category

# --- Tool Code Location ---
module: pf_reasoning_tool.tools.vision_llm_tool
function: vision_llm # Function defined in the python code

# --- Default Prompt Template ---
# Define how image input should be referenced.
# The variable name {{image_input}} here defines an expected input variable for the node.
default_prompt: |
  # system:
  You are an AI assistant. Analyze the provided image and answer the user's question based on its content.

  # user:
  {{prompt_text}}
  ![image]({{image_input}})

inputs:
  # NO 'prompt' input of type PromptTemplate here - custom_llm handles it implicitly

  connection:
    type: [AzureOpenAIConnection, OpenAIConnection, CustomConnection] # Keep CustomConnection if needed
    description: "The connection object for OpenAI or Azure OpenAI."

  # Inputs specific to Azure OpenAI Connection
  deployment_name:
    type: [string]
    description: "Deployment name for Azure OpenAI vision model."
    enabled_by: connection
    enabled_by_type: [AzureOpenAIConnection]
    allow_manual_entry: true
    optional: true # Make optional if OpenAI is also supported
    ui_hints: {text_box_size: lg}

  # Inputs specific to OpenAI Connection
  model:
    type: [string]
    description: "Model name for OpenAI vision model (e.g., gpt-4-vision-preview)."
    enabled_by: connection
    enabled_by_type: [OpenAIConnection]
    default: "gpt-4-vision-preview" # Provide a default if applicable
    optional: true # Make optional if Azure is also supported
    allow_manual_entry: true
    ui_hints: {text_box_size: lg}

  # Standard LLM Parameters (mirroring the built-in tool and your Python function)
  temperature:
    type: [number] # Use 'number' for float/double
    description: "Controls randomness (0.0 to 2.0)."
    default: 0.7 # Your preferred default
    optional: true
    ui_hints: {text_box_size: xs}

  top_p:
    type: [number]
    description: "Nucleus sampling parameter."
    default: 1.0
    optional: true
    ui_hints: {text_box_size: xs}

  stop:
    type: [list] # List of strings
    description: "Sequences where the API will stop generating."
    default: null # Use null for optional complex types
    optional: true
    allow_manual_entry: true # Allow user to type comma-separated or JSON list
    ui_hints: {text_box_size: md}

  max_tokens:
    type: [int]
    description: "Maximum number of tokens to generate."
    default: 1000 # Your preferred default, ensure it's numeric
    optional: true
    ui_hints: {text_box_size: xs}

  presence_penalty:
    type: [number]
    description: "Penalty for new token presence (-2.0 to 2.0)."
    default: 0.0
    optional: true
    ui_hints: {text_box_size: xs}

  frequency_penalty:
    type: [number]
    description: "Penalty for new token frequency (-2.0 to 2.0)."
    default: 0.0
    optional: true
    ui_hints: {text_box_size: xs}

  seed:
    type: [int]
    description: "Seed for potentially deterministic results (if supported)."
    default: null # Use null for optional numbers
    optional: true
    ui_hints: {text_box_size: xs}

  detail:
    type: [string]
    description: "Detail level for image processing ('low', 'high', 'auto')."
    enum: ["auto", "low", "high"] # Use enum for restricted choices
    default: "auto"
    optional: true

  # NOTE: The actual input variables for the template (e.g., 'prompt_text', 'image_input' from default_prompt)
  # are NOT defined here. They become dynamic inputs to the node in the flow graph based on the template used.

outputs:
  output:
    type: [string]
    description: "The text response generated by the vision model."