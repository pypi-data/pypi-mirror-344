import argparse
import sys
import os
import json
import re
import csv
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, OrderedDict
from collections import defaultdict



# å¸¸é‡å®šä¹‰
SUPPORTED_MODELS = ["gpt-3.5-turbo", "gpt-4", "qwen", "baichuan", "llama"]
DEFAULT_OUTPUT_DIR = "evalscope_results"

def main():
    parser = argparse.ArgumentParser(description='UniLawBench æ³•å¾‹è¯„ä¼°å·¥å…·')
    
    # æ”¯æŒä¸¤ç§ä½¿ç”¨æ–¹å¼ï¼šå­å‘½ä»¤æ¨¡å¼å’Œå‚æ•°æ¨¡å¼
    # 1. å­å‘½ä»¤æ¨¡å¼ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
    subparsers = parser.add_subparsers(dest='command')
    
    # Version command
    version_parser = subparsers.add_parser('version', help='æ˜¾ç¤ºå½“å‰ç‰ˆæœ¬å·')
    version_parser.set_defaults(func=handle_version)

    # GUIå‘½ä»¤
    gui_parser = subparsers.add_parser('gui', help='å¯åŠ¨å›¾å½¢ç•Œé¢')
    gui_parser.set_defaults(func=handle_gui)
    
    # è¯„ä¼°å‘½ä»¤
    eval_parser = subparsers.add_parser('eval', help='å¯åŠ¨è¯„ä¼°ä»»åŠ¡')
    eval_parser.add_argument('-set', nargs='+', help='æŒ‡å®šè¯„ä¼°æ•°æ®é›†IDï¼Œä¾‹å¦‚1-1 2-3')
    eval_parser.add_argument('-form', choices=['qa', 'mcq'], required=True, 
                           help='è¯„ä¼°å½¢å¼: qa-é—®ç­”é¢˜ mcq-é€‰æ‹©é¢˜')
    eval_parser.add_argument('-model', type=Path, required=True,
                           help='æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆåŒ…å«æ¨¡å‹æ–‡ä»¶å’Œé…ç½®æ–‡ä»¶ï¼‰')
    eval_parser.add_argument('--run-all', action='store_true',
                           help='è‡ªåŠ¨è¿è¡Œå½“å‰formç±»å‹çš„æ‰€æœ‰æ•°æ®é›†')
    eval_parser.set_defaults(func=handle_eval)

    # æ•°æ®è½¬æ¢å‘½ä»¤
    convert_parser = subparsers.add_parser('convert', help='æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·')
    convert_parser.add_argument('--type', choices=['mcq', 'qa', 'focus', 'dynamic-choice', 'evalscope'], required=True,
                              help='è½¬æ¢ç±»å‹: mcq-é€‰æ‹©é¢˜ qa-é—®ç­”é¢˜ focus-çº çº·ç„¦ç‚¹ dynamic-choice-åŠ¨æ€é€‰æ‹©é¢˜ evalscope-EvalScopeæ ¼å¼')
    convert_parser.add_argument('input', type=Path, help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    convert_parser.add_argument('output', type=Path, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    convert_parser.set_defaults(func=handle_convert)
    

    
    # å¯è§†åŒ–å‘½ä»¤
    visualize_parser = subparsers.add_parser('visualize', help='å¯è§†åŒ–è¯„ä¼°ç»“æœ')
    visualize_parser.add_argument('results', help='è¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„')
    visualize_parser.add_argument('-output', help='å¯è§†åŒ–ç»“æœè¾“å‡ºç›®å½•')
    visualize_parser.set_defaults(func=handle_visualize)
    
    # 2. å‚æ•°æ¨¡å¼ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
    # åŸºæœ¬å‚æ•°
    parser.add_argument("-set", help="æ•°æ®é›†IDï¼Œä¾‹å¦‚: 1-1, 2-2ç­‰")
    parser.add_argument("-form", choices=["qa", "mcq"], help="è¯„ä¼°å½¢å¼: qa(é—®ç­”é¢˜) æˆ– mcq(é€‰æ‹©é¢˜)")
    parser.add_argument("-model", help="æ¨¡å‹è·¯å¾„æˆ–åç§°")
    parser.add_argument("-output", help="ç»“æœè¾“å‡ºç›®å½•")
    
    # æ‰¹é‡è¿è¡Œ
    parser.add_argument("-run-all-qa", action="store_true", help="è¿è¡Œæ‰€æœ‰é—®ç­”é¢˜è¯„ä¼°")
    parser.add_argument("-run-all-mcq", action="store_true", help="è¿è¡Œæ‰€æœ‰é€‰æ‹©é¢˜è¯„ä¼°")
    
    # æ•°æ®è½¬æ¢
    parser.add_argument("-convert", choices=["jsonl2csv", "csv2jsonl", "dynamic-choice", "focus", "evalscope"], 
                        help="æ•°æ®è½¬æ¢åŠŸèƒ½")
    parser.add_argument("-input", help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-output-file", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    

    
    # å¯è§†åŒ–
    parser.add_argument("-visualize", help="å¯è§†åŒ–ç»“æœæ–‡ä»¶")
    
    # GUI
    parser.add_argument("-gui", action="store_true", help="å¯åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢")
    
    args = parser.parse_args()
    
    # å¤„ç†å­å‘½ä»¤æ¨¡å¼
    if hasattr(args, 'func'):
        if args.command == 'gui' and len(sys.argv) > 2:
            parser.error("guiå‘½ä»¤ä¸èƒ½ä¸å…¶ä»–å‚æ•°åŒæ—¶ä½¿ç”¨")
        args.func(args)
        return
    
    # å¤„ç†å‚æ•°æ¨¡å¼
    # å¯åŠ¨GUI
    if args.gui:
        handle_gui(args)
        return
    
    # å¯è§†åŒ–ç»“æœ
    if args.visualize:
        visualize_evalscope_results(args.visualize, args.output)
        return
    

    
    # æ•°æ®è½¬æ¢
    if args.convert:
        if not args.input or not args.output_file:
            print("é”™è¯¯: æ•°æ®è½¬æ¢éœ€è¦æŒ‡å®š -input å’Œ -output-file å‚æ•°")
            return
        
        if args.convert == "evalscope":
            convert_to_evalscope_format(args.input, args.output_file)
        else:
            from .utils.data_converter import convert_format
            convert_format(Path(args.input), Path(args.output_file), args.convert)
        return
    
    # è¿è¡Œæ‰€æœ‰é—®ç­”é¢˜
    if args.run_all_qa:
        if not args.model:
            print("é”™è¯¯: è¯·æŒ‡å®š -model å‚æ•°")
            return
        
        datasets = get_all_datasets("qa")
        for dataset_path in datasets:
            evaluate_dataset(args.model, str(dataset_path), args.output)
        return
    
    # è¿è¡Œæ‰€æœ‰é€‰æ‹©é¢˜
    if args.run_all_mcq:
        if not args.model:
            print("é”™è¯¯: è¯·æŒ‡å®š -model å‚æ•°")
            return
        
        datasets = get_all_datasets("mcq")
        for dataset_path in datasets:
            evaluate_dataset(args.model, str(dataset_path), args.output)
        return
    
    # å•ä¸ªæ•°æ®é›†è¯„ä¼°
    if args.set and args.form and args.model:
        try:
            dataset_path = get_dataset_path(args.set, args.form)
            evaluate_dataset(args.model, str(dataset_path), args.output)
        except FileNotFoundError as e:
            print(f"é”™è¯¯: {e}")
        return
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…ä»»ä½•å‘½ä»¤ï¼Œæ˜¾ç¤ºå¸®åŠ©
    parser.print_help()

def handle_convert(args):
    try:
        if args.type == 'evalscope':
            convert_to_evalscope_format(args.input, args.output)
        elif args.type == 'dynamic-choice':
            convert_dynamic_choice(args.input, args.output)
        else:
            # å†…è”æ•°æ®è½¬æ¢å‡½æ•°
            if args.type == 'focus':
                convert_focus(args.input, args.output)
            elif args.type == 'mcq':
                convert_mcq(args.input, args.output)
            elif args.type == 'qa':
                convert_qa(args.input, args.output)
        print(f"âœ… è½¬æ¢å®Œæˆ: {args.output}")
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {str(e)}")
        exit(1)

# æ•°æ®è½¬æ¢å‡½æ•°å®ç°
def convert_focus(input_path, output_path):
    records, bad_lines = load_records(input_path)
    with output_path.open('w', encoding='utf-8') as f:
        for rec in records:
            f.write(json.dumps({
                "instruction": "åˆ¤æ–­å¥å­åŒ…å«çš„äº‰è®®ç„¦ç‚¹ç±»åˆ«ï¼Œæ¯ä¸ªå¥å­åªåŒ…å«ä¸€ä¸ªäº‰è®®ç„¦ç‚¹ç±»åˆ«ã€‚",
                "question": rec["question"],
                "answer": f"æ­£ç¡®ç­”æ¡ˆï¼š{'ã€'.join(extract_categories(rec['answer']))}ã€‚"
            }) + '\n')

def convert_mcq(input_path, output_path):
    cats = extract_categories_from_first_line(input_path)
    rows = [parse_item(obj, cats) for obj in load_jsonl(input_path)]
    
    with output_path.open('w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['id', 'question'] + list(cats.values()) + ['answer'])
        for idx, r in enumerate(rows, 1):
            writer.writerow([idx, r['question']] + [cats[l] for l in cats] + [r['answer']])

def convert_qa(input_path, output_path):
    with input_path.open(encoding='utf-8') as fin, output_path.open('w', encoding='utf-8') as fout:
        for line in fin:
            data = json.loads(line)
            fout.write(json.dumps({
                "instruction": data.get("instruction", ""),
                "question": data["question"],
                "answer": data["answer"]
            }) + '\n')

def convert_to_evalscope_format(input_file: str, output_file: str) -> None:
    """å°†UniLawBenchæ•°æ®è½¬æ¢ä¸ºEvalScopeæ ¼å¼"""
    input_path = Path(input_file)
    output_path = Path(output_file)
    
    if not input_path.exists():
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šå¤„ç†æ–¹å¼
    if input_path.suffix.lower() == ".jsonl":
        _convert_jsonl_to_evalscope(input_path, output_path)
    elif input_path.suffix.lower() == ".csv":
        _convert_csv_to_evalscope(input_path, output_path)
    else:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ {input_path.suffix}")
        return
    
    print(f"âœ… å·²å°† {input_file} è½¬æ¢ä¸ºEvalScopeæ ¼å¼: {output_file}")

def _convert_jsonl_to_evalscope(input_path: Path, output_path: Path) -> None:
    """å°†JSONLæ ¼å¼è½¬æ¢ä¸ºEvalScopeæ ¼å¼"""
    evalscope_data = []
    
    with open(input_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            
            try:
                item = json.loads(line)
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯EvalScopeæ ¼å¼
                if "choices" in item and "answer_index" in item:
                    evalscope_data.append(item)
                    continue
                
                # ä»instructionä¸­æå–é€‰é¡¹
                instruction = item.get("instruction", "")
                question = item.get("question", "")
                answer = item.get("answer", "")
                
                # è§£æé€‰é¡¹å’Œç­”æ¡ˆ
                choices, answer_indices = _parse_options_and_answer(instruction, answer)
                
                # åˆ›å»ºEvalScopeæ ¼å¼çš„æ•°æ®é¡¹
                evalscope_item = {
                    "question": question,
                    "choices": choices,
                    "answer_index": answer_indices,
                    "metadata": {
                        "original_instruction": instruction,
                        "original_answer": answer
                    }
                }
                
                evalscope_data.append(evalscope_item)
                
            except json.JSONDecodeError:
                print(f"è­¦å‘Š: è·³è¿‡æ— æ•ˆçš„JSONè¡Œ: {line[:50]}...")
            except Exception as e:
                print(f"è­¦å‘Š: å¤„ç†è¡Œæ—¶å‡ºé”™: {str(e)}")
    
    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    with open(output_path, "w", encoding="utf-8") as f:
        for item in evalscope_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

def _convert_csv_to_evalscope(input_path: Path, output_path: Path) -> None:
    """å°†CSVæ ¼å¼è½¬æ¢ä¸ºEvalScopeæ ¼å¼"""
    evalscope_data = []
    
    with open(input_path, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        headers = reader.fieldnames
        
        if not headers or "question" not in headers or "answer" not in headers:
            print("é”™è¯¯: CSVæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œå¿…é¡»åŒ…å«'question'å’Œ'answer'åˆ—")
            return
        
        # ç¡®å®šé€‰é¡¹åˆ—
        option_columns = [h for h in headers if h not in ["id", "question", "answer"]]
        
        for row in reader:
            question = row.get("question", "")
            answer = row.get("answer", "")
            
            # æå–é€‰é¡¹
            choices = [row.get(col, "") for col in option_columns if row.get(col)]
            
            # è§£æç­”æ¡ˆï¼ˆæ ¼å¼å¦‚"Aã€B"ï¼‰
            answer_letters = [letter.strip() for letter in answer.split("ã€") if letter.strip()]
            answer_indices = [ord(letter) - ord("A") for letter in answer_letters if "A" <= letter <= "Z"]
            
            evalscope_item = {
                "question": question,
                "choices": choices,
                "answer_index": answer_indices,
                "metadata": {
                    "original_answer": answer
                }
            }
            
            evalscope_data.append(evalscope_item)
    
    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    with open(output_path, "w", encoding="utf-8") as f:
        for item in evalscope_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

def _parse_options_and_answer(instruction: str, answer: str) -> tuple:
    """ä»instructionå’Œanswerä¸­è§£æé€‰é¡¹å’Œç­”æ¡ˆ"""
    # ä»instructionä¸­æå–é€‰é¡¹
    options_pattern = r"([A-Z])ã€([^;ï¼›]+)"
    options_matches = re.findall(options_pattern, instruction)
    
    if not options_matches:
        raise ValueError("æ— æ³•ä»instructionä¸­æå–é€‰é¡¹")
    
    # æ„å»ºé€‰é¡¹åˆ—è¡¨
    choices = [text.strip() for _, text in options_matches]
    
    # ä»answerä¸­æå–ç­”æ¡ˆå­—æ¯
    answer_pattern = r"[A-Z]"
    answer_letters = re.findall(answer_pattern, answer)
    
    if not answer_letters:
        raise ValueError(f"æ— æ³•ä»answerä¸­æå–ç­”æ¡ˆå­—æ¯: {answer}")
    
    # å°†ç­”æ¡ˆå­—æ¯è½¬æ¢ä¸ºç´¢å¼•
    answer_indices = [ord(letter) - ord("A") for letter in answer_letters]
    
    return choices, answer_indices

# ================= è¾…åŠ©å‡½æ•° =================
def get_dataset_path(dataset_id: str, form: str) -> Path:
    """è·å–æ•°æ®é›†æ–‡ä»¶è·¯å¾„"""
    # ç¡®å®šæ•°æ®é›†ç›®å½•
    if form.lower() == "qa":
        base_dir = "qa"
        ext = ".jsonl"
    elif form.lower() == "mcq":
        base_dir = "mcq"
        ext = ".csv"
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å½¢å¼: {form}ï¼Œè¯·ä½¿ç”¨ 'qa' æˆ– 'mcq'")
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ•°æ®é›†æ–‡ä»¶
    dataset_dir = Path(__file__).parent / "dataset" / base_dir
    
    # å¦‚æœæ˜¯ç²¾ç¡®åŒ¹é…ï¼ˆå¦‚1-1ï¼‰
    exact_match = list(dataset_dir.glob(f"{dataset_id}*{ext}"))
    if exact_match:
        return exact_match[0]
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›é”™è¯¯
    raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®é›†: {dataset_id} (å½¢å¼: {form})")

def get_all_datasets(form: str) -> List[Path]:
    """è·å–æŒ‡å®šå½¢å¼çš„æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶è·¯å¾„"""
    if form.lower() == "qa":
        base_dir = "qa"
        ext = ".jsonl"
    elif form.lower() == "mcq":
        base_dir = "mcq"
        ext = ".csv"
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å½¢å¼: {form}ï¼Œè¯·ä½¿ç”¨ 'qa' æˆ– 'mcq'")
    
    dataset_dir = Path(__file__).parent / "dataset" / base_dir
    return list(dataset_dir.glob(f"*{ext}"))

# ================= è¯„ä¼°å‡½æ•° =================
def evaluate_dataset(model_name: str, dataset_path: str, 
                    output_dir: Optional[str] = None) -> str:
    """è¯„ä¼°æ¨¡å‹åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šçš„è¡¨ç°"""
    if model_name not in SUPPORTED_MODELS:
        print(f"è­¦å‘Š: æ¨¡å‹ {model_name} å¯èƒ½ä¸å—æ”¯æŒã€‚æ”¯æŒçš„æ¨¡å‹: {', '.join(SUPPORTED_MODELS)}")
    
    dataset_path = Path(dataset_path)
    if not dataset_path.exists():
        print(f"é”™è¯¯: æ•°æ®é›†æ–‡ä»¶ {dataset_path} ä¸å­˜åœ¨")
        return ""
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = DEFAULT_OUTPUT_DIR
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆç»“æœæ–‡ä»¶è·¯å¾„
    result_file = Path(output_dir) / f"{model_name}_{dataset_path.stem}_results.json"
    
    try:
        # è°ƒç”¨EvalScopeè¿›è¡Œè¯„ä¼°
        print(f"å¼€å§‹ä½¿ç”¨ {model_name} è¯„ä¼°æ•°æ®é›† {dataset_path}...")
        results = evaluate_model(
            model_name=model_name,
            dataset_path=str(dataset_path),
            output_file=str(result_file)
        )
        
        print(f"âœ… è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {result_file}")
        return str(result_file)
        
    except DatasetFormatError as e:
        print(f"é”™è¯¯: æ•°æ®é›†æ ¼å¼ä¸æ­£ç¡®: {e}")
    except Exception as e:
        print(f"é”™è¯¯: è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    return ""

# ================= å¯è§†åŒ–å‡½æ•° =================
def visualize_evalscope_results(results_file: str, output_dir: Optional[str] = None) -> None:
    """å¯è§†åŒ–EvalScopeè¯„ä¼°ç»“æœ"""
    results_path = Path(results_file)
    if not results_path.exists():
        print(f"é”™è¯¯: ç»“æœæ–‡ä»¶ {results_file} ä¸å­˜åœ¨")
        return
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = results_path.parent / "visualizations"
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # è°ƒç”¨EvalScopeè¿›è¡Œå¯è§†åŒ–
        print(f"å¼€å§‹å¯è§†åŒ–ç»“æœæ–‡ä»¶ {results_file}...")
        visualize_results(
            results_file=str(results_path),
            output_dir=str(output_dir)
        )
        
        print(f"âœ… å¯è§†åŒ–å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_dir}")
        
    except Exception as e:
        print(f"é”™è¯¯: å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

# ================= EvalScopeé›†æˆå‡½æ•° =================
def evaluate_with_evalscope(model_name: str, dataset_path: str, 
                           output_dir: Optional[str] = None) -> str:
    """ä½¿ç”¨EvalScopeè¯„ä¼°æ¨¡å‹åœ¨UniLawBenchä¸Šçš„è¡¨ç°"""
    if model_name not in SUPPORTED_MODELS:
        print(f"è­¦å‘Š: æ¨¡å‹ {model_name} å¯èƒ½ä¸å—æ”¯æŒã€‚æ”¯æŒçš„æ¨¡å‹: {', '.join(SUPPORTED_MODELS)}")
    
    dataset_path = Path(dataset_path)
    if not dataset_path.exists():
        print(f"é”™è¯¯: æ•°æ®é›†æ–‡ä»¶ {dataset_path} ä¸å­˜åœ¨")
        return ""
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = DEFAULT_OUTPUT_DIR
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆç»“æœæ–‡ä»¶è·¯å¾„
    result_file = Path(output_dir) / f"{model_name}_results.json"
    
    try:
        # è°ƒç”¨EvalScopeè¿›è¡Œè¯„ä¼°
        print(f"å¼€å§‹ä½¿ç”¨ {model_name} è¯„ä¼°æ•°æ®é›† {dataset_path}...")
        results = evaluate_model(
            model_name=model_name,
            dataset_path=str(dataset_path),
            output_file=str(result_file)
        )
        
        print(f"âœ… è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {result_file}")
        return str(result_file)
        
    except DatasetFormatError as e:
        print(f"é”™è¯¯: æ•°æ®é›†æ ¼å¼ä¸æ­£ç¡®: {e}")
    except Exception as e:
        print(f"é”™è¯¯: è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    return ""


def resolve_datasets(args) -> List[str]:
    """è§£ææ•°æ®é›†å‚æ•°ï¼Œæ”¯æŒ--run-allæ—¶è‡ªåŠ¨åŠ è½½å¯¹åº”ç±»å‹çš„æ‰€æœ‰æ•°æ®é›†"""
    if args.run_all:
        data_dir = Path(__file__).parent / 'dataset' / args.form
        return [f.stem for f in data_dir.glob('*.*') if f.suffix in ('.csv', '.jsonl')]
    return args.set if args.set else []

def handle_evalscope(args):
    """å¤„ç†EvalScopeè¯„ä¼°å‘½ä»¤"""
    try:
        result_file = evaluate_with_evalscope(
            model_name=args.model,
            dataset_path=args.dataset,
            output_dir=args.output
        )
        if result_file:
            print(f"âœ… EvalScopeè¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {result_file}")
    except Exception as e:
        print(f"âŒ EvalScopeè¯„ä¼°å¤±è´¥: {str(e)}")
        exit(1)

def handle_visualize(args):
    """å¤„ç†å¯è§†åŒ–å‘½ä»¤"""
    try:
        visualize_evalscope_results(args.results, args.output)
        print(f"âœ… å¯è§†åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ å¯è§†åŒ–å¤±è´¥: {str(e)}")
        exit(1)
        
def handle_test(args):
    """å¤„ç†æµ‹è¯•å‘½ä»¤"""
    import subprocess
    
    test_cmd = ["python", "-m", "unittest", "discover", "tests"]
    if args.coverage:
        test_cmd = ["coverage", "run", "-m", "unittest", "discover", "tests"]
    
    try:
        subprocess.run(test_cmd, check=True)
        if args.coverage:
            subprocess.run(["coverage", "report"], check=True)
    except subprocess.CalledProcessError as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)

def convert_dynamic_choice(input_file: Path, output_file: Path) -> None:
    """å°†åŠ¨æ€é€‰æ‹©é¢˜è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼"""
    try:
        # è¯»å–è¾“å…¥æ–‡ä»¶
        if input_file.suffix.lower() == ".jsonl":
            with open(input_file, "r", encoding="utf-8") as f:
                data = [json.loads(line) for line in f if line.strip()]
        elif input_file.suffix.lower() in [".json"]:
            with open(input_file, "r", encoding="utf-8") as f:
                text = f.read().lstrip()
                if text.startswith("["):
                    data = json.loads(text)
                else:
                    data = [json.loads(line) for line in text.split("\n") if line.strip()]
        else:
            print(f"é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ {input_file.suffix}")
            return
        
        # å¤„ç†æ¯æ¡è®°å½•
        converted_data = []
        for item in data:
            # ä»instructionä¸­æå–æ ‡ç­¾
            instruction = item.get("instruction", "")
            question = item.get("question", "")
            answer = item.get("answer", "")
            
            # æå–æ ‡ç­¾åˆ—è¡¨
            match = re.search(r"æ ‡ç­¾åŒ…æ‹¬ï¼š(.+?)ã€‚", instruction)
            if not match:
                print(f"è­¦å‘Š: æ— æ³•ä»instructionä¸­æå–æ ‡ç­¾åˆ—è¡¨: {instruction[:50]}...")
                continue
                
            labels_text = match.group(1)
            labels = [lab.strip() for lab in re.split(r"[ã€,ï¼Œ;ï¼›\s]+", labels_text) if lab.strip()]
            
            # åˆ›å»ºæ ‡ç­¾åˆ°å­—æ¯çš„æ˜ å°„
            label_to_letter = {label: chr(65 + i) for i, label in enumerate(labels)}
            
            # ä»answerä¸­æå–æ ‡ç­¾
            answer_labels = []
            
            # å°è¯•ä¸åŒæ ¼å¼
            # æ ¼å¼1: "ç±»åˆ«: xxx"
            match = re.search(r"ç±»åˆ«[:ï¼š]\s*([^\.;ã€‚]+)", answer)
            if match:
                answer_labels = [t.strip() for t in re.split(r"[ã€,ï¼Œ;\s]+", match.group(1)) if t.strip()]
            
            # æ ¼å¼2: "[ç±»åˆ«]xxx<eoa>"
            if not answer_labels:
                match = re.search(r"\[ç±»åˆ«\]\s*([^\[<]+?)<\s*eoa\s*>", answer, flags=re.I)
                if match:
                    answer_labels = [t.strip() for t in re.split(r"[ã€,ï¼Œ;\s]+", match.group(1)) if t.strip()]
            
            # æ ¼å¼3: "æ­£ç¡®ç­”æ¡ˆï¼šAã€Bã€‚"
            if not answer_labels:
                match = re.search(r"æ­£ç¡®ç­”æ¡ˆ[:ï¼š]\s*([A-Z](?:[ã€,ï¼Œ;][A-Z])*)", answer, flags=re.I)
                if match:
                    answer_letters = [s.strip().upper() for s in re.split(r"[ã€,ï¼Œ;]", match.group(1))]
                    # åå‘æŸ¥æ‰¾æ ‡ç­¾
                    letter_to_label = {v: k for k, v in label_to_letter.items()}
                    answer_labels = [letter_to_label.get(letter, "") for letter in answer_letters if letter in letter_to_label]
            
            # æ ¼å¼4: ç›´æ¥ä½¿ç”¨ä¸­æ–‡æ ‡ç­¾
            if not answer_labels:
                potential_labels = [t.strip() for t in re.split(r"[ã€,ï¼Œ;\s]+", answer) if t.strip()]
                answer_labels = [label for label in potential_labels if label in label_to_letter]
            
            if not answer_labels:
                print(f"è­¦å‘Š: æ— æ³•ä»answerä¸­æå–æ ‡ç­¾: {answer}")
                continue
            
            # è·å–ç­”æ¡ˆå­—æ¯
            answer_letters = [label_to_letter[label] for label in answer_labels if label in label_to_letter]
            answer_letters = sorted(set(answer_letters))
            
            # åˆ›å»ºæ–°çš„instruction
            parts = [f"{letter}ã€{label}" for label, letter in label_to_letter.items()]
            new_instruction = f"åˆ¤æ–­å¥å­æ‰€å±ç±»åˆ«ï¼Œå¯ä¸ºå•é€‰æˆ–å¤šé€‰ã€‚ç±»åˆ«åŒ…æ‹¬:{';'.join(parts)}ã€‚é€‰æ‹©æ­£ç¡®çš„ç­”æ¡ˆã€‚"
            
            # åˆ›å»ºæ–°è®°å½•
            converted_item = {
                "instruction": new_instruction,
                "question": question,
                "answer": f"æ­£ç¡®ç­”æ¡ˆï¼š{'ã€'.join(answer_letters)}ã€‚"
            }
            
            converted_data.append(converted_item)
        
        # å†™å…¥è¾“å‡ºæ–‡ä»¶
        with open(output_file, "w", encoding="utf-8") as f:
            for item in converted_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        print(f"âœ… å·²å°† {len(converted_data)} æ¡è®°å½•è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼: {output_file}")
        
    except Exception as e:
        print(f"âŒ åŠ¨æ€é€‰æ‹©é¢˜è½¬æ¢å¤±è´¥: {str(e)}")


def handle_gui(args):
    from .gui.window import MainWindow
    from PyQt5.QtWidgets import QApplication
    
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    sys.exit(app.exec_())

def handle_eval(args):
    from .utils.evaluator import run_evaluation
    try:
        datasets = resolve_datasets(args)
        print(f"ğŸš€ å¼€å§‹è¯„ä¼°: {args.form} å½¢å¼, å…± {len(datasets)} ä¸ªæ•°æ®é›†")
        
        results = run_evaluation(
            model_path=args.model,
            dataset_ids=datasets,
            eval_type=args.form
        )
        
        print("\nâœ… è¯„ä¼°å®Œæˆ:")
        for dataset_id, metrics in results.items():
            print(f"ğŸ“Š {dataset_id}: {metrics}")
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
        exit(1)

def handle_version(args):
    from unilawbench import __version__
    print(f"UniLawBench ç‰ˆæœ¬: {__version__}")

if __name__ == '__main__':
    main()