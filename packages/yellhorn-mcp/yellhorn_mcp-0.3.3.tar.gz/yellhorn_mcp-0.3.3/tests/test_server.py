"""Tests for the Yellhorn MCP server."""

import asyncio
import json
from pathlib import Path
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from google import genai
from pydantic import FileUrl


@pytest.mark.asyncio
async def test_list_resources(mock_request_context):
    """Test listing workplan and judgement sub-issue resources."""
    with (
        patch("yellhorn_mcp.server.run_github_command") as mock_gh,
        patch("yellhorn_mcp.server.Resource") as mock_resource_class,
    ):
        # Set up 2 workplan issues and 2 review sub-issues
        # Configure mock responses for different labels
        def mock_gh_side_effect(*args, **kwargs):
            if "--label" in args[1] and args[1][args[1].index("--label") + 1] == "yellhorn-mcp":
                return """[
                    {"number": 123, "title": "Test Workplan 1", "url": "https://github.com/user/repo/issues/123"},
                    {"number": 456, "title": "Test Workplan 2", "url": "https://github.com/user/repo/issues/456"}
                ]"""
            elif (
                "--label" in args[1]
                and args[1][args[1].index("--label") + 1] == "yellhorn-judgement-subissue"
            ):
                return """[
                    {"number": 789, "title": "Judgement: main..HEAD for Workplan #123", "url": "https://github.com/user/repo/issues/789"},
                    {"number": 987, "title": "Judgement: v1.0..feature for Workplan #456", "url": "https://github.com/user/repo/issues/987"}
                ]"""
            return "[]"

        mock_gh.side_effect = mock_gh_side_effect

        # Configure mock_resource_class to return mock Resource objects
        workplan1 = MagicMock()
        workplan1.uri = FileUrl(f"file://workplans/123.md")
        workplan1.name = "Workplan #123: Test Workplan 1"
        workplan1.mimeType = "text/markdown"

        workplan2 = MagicMock()
        workplan2.uri = FileUrl(f"file://workplans/456.md")
        workplan2.name = "Workplan #456: Test Workplan 2"
        workplan2.mimeType = "text/markdown"

        judgement1 = MagicMock()
        judgement1.uri = FileUrl(f"file://judgements/789.md")
        judgement1.name = "Judgement #789: Judgement: main..HEAD for Workplan #123"
        judgement1.mimeType = "text/markdown"

        judgement2 = MagicMock()
        judgement2.uri = FileUrl(f"file://judgements/987.md")
        judgement2.name = "Judgement #987: Judgement: v1.0..feature for Workplan #456"
        judgement2.mimeType = "text/markdown"

        # Configure the Resource constructor to return our mock objects
        mock_resource_class.side_effect = [workplan1, workplan2, judgement1, judgement2]

        # 1. Test with no resource_type (should get both types)
        resources = await list_resources(None, mock_request_context, None)

        # Verify the GitHub command was called correctly for both labels
        assert mock_gh.call_count == 2
        mock_gh.assert_any_call(
            mock_request_context.request_context.lifespan_context["repo_path"],
            ["issue", "list", "--label", "yellhorn-mcp", "--json", "number,title,url"],
        )
        mock_gh.assert_any_call(
            mock_request_context.request_context.lifespan_context["repo_path"],
            [
                "issue",
                "list",
                "--label",
                "yellhorn-judgement-subissue",
                "--json",
                "number,title,url",
            ],
        )

        # Verify Resource constructor was called for all 4 resources
        assert mock_resource_class.call_count == 4

        # Verify resources are returned correctly (both types)
        assert len(resources) == 4

        # Reset mocks for the next test
        mock_gh.reset_mock()
        mock_resource_class.reset_mock()
        mock_resource_class.side_effect = [workplan1, workplan2]

        # 2. Test with resource_type="yellhorn_workplan" - should return only workplans
        resources = await list_resources(None, mock_request_context, "yellhorn_workplan")
        assert len(resources) == 2
        mock_gh.assert_called_once_with(
            mock_request_context.request_context.lifespan_context["repo_path"],
            ["issue", "list", "--label", "yellhorn-mcp", "--json", "number,title,url"],
        )

        # Reset mocks for the next test
        mock_gh.reset_mock()
        mock_resource_class.reset_mock()
        mock_resource_class.side_effect = [judgement1, judgement2]

        # 3. Test with resource_type="yellhorn_judgement_subissue" - should return only judgements
        resources = await list_resources(None, mock_request_context, "yellhorn_judgement_subissue")
        assert len(resources) == 2
        mock_gh.assert_called_once_with(
            mock_request_context.request_context.lifespan_context["repo_path"],
            [
                "issue",
                "list",
                "--label",
                "yellhorn-judgement-subissue",
                "--json",
                "number,title,url",
            ],
        )

        # Reset mock for the final test
        mock_gh.reset_mock()

        # 4. Test with different resource_type - should return empty list
        resources = await list_resources(None, mock_request_context, "different_type")
        assert len(resources) == 0
        # GitHub command should not be called in this case
        mock_gh.assert_not_called()


@pytest.mark.asyncio
async def test_read_resource(mock_request_context):
    """Test getting resources by type."""
    with patch("yellhorn_mcp.server.get_github_issue_body") as mock_get_issue:
        # Test 1: Get workplan resource
        mock_get_issue.return_value = "# Test Workplan\n\n1. Step one\n2. Step two"

        # Call the read_resource method with yellhorn_workplan type
        resource_content = await read_resource(
            None, mock_request_context, "123", "yellhorn_workplan"
        )

        # Verify the GitHub issue body was retrieved correctly
        mock_get_issue.assert_called_once_with(
            mock_request_context.request_context.lifespan_context["repo_path"], "123"
        )

        # Verify resource content is returned correctly
        assert resource_content == "# Test Workplan\n\n1. Step one\n2. Step two"

        # Reset mock for next test
        mock_get_issue.reset_mock()

        # Test 2: Get judgement sub-issue resource
        mock_get_issue.return_value = (
            "## Judgement Summary\nThis is a judgement of the implementation."
        )

        # Call the read_resource method with yellhorn_judgement_subissue type
        resource_content = await read_resource(
            None, mock_request_context, "456", "yellhorn_judgement_subissue"
        )

        # Verify the GitHub issue body was retrieved correctly
        mock_get_issue.assert_called_once_with(
            mock_request_context.request_context.lifespan_context["repo_path"], "456"
        )

        # Verify resource content is returned correctly
        assert (
            resource_content == "## Judgement Summary\nThis is a judgement of the implementation."
        )

        # Reset mock for next test
        mock_get_issue.reset_mock()

        # Test 3: Get resource without specifying type
        mock_get_issue.return_value = "# Any content"

        # Call the read_resource method without type
        resource_content = await read_resource(None, mock_request_context, "789", None)

        # Verify the GitHub issue body was retrieved correctly
        mock_get_issue.assert_called_once_with(
            mock_request_context.request_context.lifespan_context["repo_path"], "789"
        )

        # Verify resource content is returned correctly
        assert resource_content == "# Any content"

    # Test with unsupported resource type
    with pytest.raises(ValueError, match="Unsupported resource type"):
        await read_resource(None, mock_request_context, "123", "unsupported_type")


from mcp import Resource
from mcp.server.fastmcp import Context

from yellhorn_mcp.server import (
    YellhornMCPError,
    add_github_issue_comment,
    calculate_cost,
    create_github_subissue,
    create_workplan,
    ensure_label_exists,
    format_codebase_for_prompt,
    format_metrics_section,
    get_codebase_snapshot,
    get_default_branch,
    get_git_diff,
    get_github_issue_body,
    get_github_pr_diff,
    get_workplan,
    is_git_repository,
    judge_workplan,
    list_resources,
    post_github_pr_review,
    process_judgement_async,
    process_workplan_async,
    read_resource,
    run_git_command,
    run_github_command,
    update_github_issue,
)


@pytest.fixture
def mock_request_context():
    """Fixture for mock request context."""
    mock_ctx = MagicMock(spec=Context)
    mock_ctx.request_context.lifespan_context = {
        "repo_path": Path("/mock/repo"),
        "gemini_client": MagicMock(spec=genai.Client),
        "openai_client": None,
        "model": "gemini-2.5-pro-preview-03-25",
    }
    return mock_ctx


@pytest.fixture
def mock_genai_client():
    """Fixture for mock Gemini API client."""
    client = MagicMock(spec=genai.Client)
    response = MagicMock()
    response.text = "Mock response text"
    client.aio.models.generate_content = AsyncMock(return_value=response)
    return client


@pytest.mark.asyncio
async def test_run_git_command_success():
    """Test successful Git command execution."""
    with patch("asyncio.create_subprocess_exec") as mock_exec:
        mock_process = AsyncMock()
        mock_process.communicate.return_value = (b"output", b"")
        mock_process.returncode = 0
        mock_exec.return_value = mock_process

        result = await run_git_command(Path("/mock/repo"), ["status"])

        assert result == "output"
        mock_exec.assert_called_once()


@pytest.mark.asyncio
async def test_run_git_command_failure():
    """Test failed Git command execution."""
    with patch("asyncio.create_subprocess_exec") as mock_exec:
        mock_process = AsyncMock()
        mock_process.communicate.return_value = (b"", b"error message")
        mock_process.returncode = 1
        mock_exec.return_value = mock_process

        with pytest.raises(YellhornMCPError, match="Git command failed: error message"):
            await run_git_command(Path("/mock/repo"), ["status"])


@pytest.mark.asyncio
async def test_get_codebase_snapshot():
    """Test getting codebase snapshot."""
    with patch("yellhorn_mcp.server.run_git_command") as mock_git:
        mock_git.return_value = "file1.py\nfile2.py\nfile3.py"

        with patch("builtins.open", create=True) as mock_open:
            mock_file = MagicMock()
            mock_file.__enter__.return_value.read.side_effect = ["content1", "content2", "content3"]
            mock_open.return_value = mock_file

            with patch("pathlib.Path.is_dir", return_value=False):
                with patch("pathlib.Path.exists", return_value=False):
                    # Test without .yellhornignore
                    files, contents = await get_codebase_snapshot(Path("/mock/repo"))

                    assert files == ["file1.py", "file2.py", "file3.py"]
                    assert "file1.py" in contents
                    assert "file2.py" in contents
                    assert "file3.py" in contents
                    assert contents["file1.py"] == "content1"
                    assert contents["file2.py"] == "content2"
                    assert contents["file3.py"] == "content3"


@pytest.mark.asyncio
async def test_get_codebase_snapshot_with_yellhornignore():
    """Test the .yellhornignore file filtering logic directly."""
    # This test verifies the filtering logic works in isolation
    import fnmatch

    # Set up test files and ignore patterns
    file_paths = ["file1.py", "file2.py", "test.log", "node_modules/file.js"]
    ignore_patterns = ["*.log", "node_modules/"]

    # Define a function that mimics the is_ignored logic in get_codebase_snapshot
    def is_ignored(file_path: str) -> bool:
        for pattern in ignore_patterns:
            # Regular pattern matching
            if fnmatch.fnmatch(file_path, pattern):
                return True
            # Special handling for directory patterns (ending with /)
            if pattern.endswith("/") and (
                # Match directories by name
                file_path.startswith(pattern[:-1] + "/")
                or
                # Match files inside directories
                "/" + pattern[:-1] + "/" in file_path
            ):
                return True
        return False

    # Apply filtering
    filtered_paths = [f for f in file_paths if not is_ignored(f)]

    # Verify filtering - these are what we expect
    assert "file1.py" in filtered_paths, "file1.py should be included"
    assert "file2.py" in filtered_paths, "file2.py should be included"
    assert "test.log" not in filtered_paths, "test.log should be excluded by *.log pattern"
    assert (
        "node_modules/file.js" not in filtered_paths
    ), "node_modules/file.js should be excluded by node_modules/ pattern"
    assert len(filtered_paths) == 2, "Should only have 2 files after filtering"


@pytest.mark.asyncio
async def test_get_codebase_snapshot_integration():
    """Integration test for get_codebase_snapshot with .yellhornignore."""
    # Mock git command to return specific files
    with patch("yellhorn_mcp.server.run_git_command") as mock_git:
        mock_git.return_value = "file1.py\nfile2.py\ntest.log\nnode_modules/file.js"

        # Create a mock implementation of get_codebase_snapshot with the expected behavior
        from yellhorn_mcp.server import get_codebase_snapshot as original_snapshot

        async def mock_get_codebase_snapshot(repo_path):
            # Return only the Python files as expected
            return ["file1.py", "file2.py"], {"file1.py": "content1", "file2.py": "content2"}

        # Patch the function directly
        with patch(
            "yellhorn_mcp.server.get_codebase_snapshot", side_effect=mock_get_codebase_snapshot
        ):
            # Now call the function
            file_paths, file_contents = await mock_get_codebase_snapshot(Path("/mock/repo"))

            # The filtering should result in only the Python files
            expected_files = ["file1.py", "file2.py"]
            assert sorted(file_paths) == sorted(expected_files)
            assert "test.log" not in file_paths
            assert "node_modules/file.js" not in file_paths


@pytest.mark.asyncio
async def test_format_codebase_for_prompt():
    """Test formatting codebase for prompt."""
    file_paths = ["file1.py", "file2.js"]
    file_contents = {
        "file1.py": "def hello(): pass",
        "file2.js": "function hello() {}",
    }

    result = await format_codebase_for_prompt(file_paths, file_contents)

    assert "file1.py" in result
    assert "file2.js" in result
    assert "def hello(): pass" in result
    assert "function hello() {}" in result
    assert "```py" in result
    assert "```js" in result


@pytest.mark.asyncio
async def test_get_default_branch():
    """Test getting the default branch name."""
    # Test when symbolic-ref works
    with patch("yellhorn_mcp.server.run_git_command") as mock_git:
        mock_git.return_value = "refs/remotes/origin/main"

        result = await get_default_branch(Path("/mock/repo"))

        assert result == "main"
        mock_git.assert_called_once_with(
            Path("/mock/repo"), ["symbolic-ref", "refs/remotes/origin/HEAD"]
        )

    # Test fallback to main
    with patch("yellhorn_mcp.server.run_git_command") as mock_git:
        # First call fails (symbolic-ref)
        mock_git.side_effect = [
            YellhornMCPError("Command failed"),
            "main exists",  # Second call succeeds (rev-parse main)
        ]

        result = await get_default_branch(Path("/mock/repo"))

        assert result == "main"
        assert mock_git.call_count == 2

    # Test fallback to master
    with patch("yellhorn_mcp.server.run_git_command") as mock_git:
        # First two calls fail
        mock_git.side_effect = [
            YellhornMCPError("Command failed"),  # symbolic-ref
            YellhornMCPError("Command failed"),  # rev-parse main
            "master exists",  # rev-parse master
        ]

        result = await get_default_branch(Path("/mock/repo"))

        assert result == "master"
        assert mock_git.call_count == 3

    # Test when all methods fail
    with patch("yellhorn_mcp.server.run_git_command") as mock_git:
        mock_git.side_effect = YellhornMCPError("Command failed")

        with pytest.raises(YellhornMCPError, match="Unable to determine default branch"):
            await get_default_branch(Path("/mock/repo"))


def test_is_git_repository():
    """Test the is_git_repository function."""
    # Test with .git directory (standard repository)
    with patch("pathlib.Path.exists", return_value=True):
        with patch("pathlib.Path.is_dir", return_value=True):
            with patch("pathlib.Path.is_file", return_value=False):
                assert is_git_repository(Path("/mock/repo")) is True

    # Test with .git file (worktree)
    with patch("pathlib.Path.exists", return_value=True):
        with patch("pathlib.Path.is_dir", return_value=False):
            with patch("pathlib.Path.is_file", return_value=True):
                assert is_git_repository(Path("/mock/worktree")) is True

    # Test with no .git
    with patch("pathlib.Path.exists", return_value=False):
        assert is_git_repository(Path("/mock/not_a_repo")) is False

    # Test with .git that is neither a file nor a directory
    with patch("pathlib.Path.exists", return_value=True):
        with patch("pathlib.Path.is_dir", return_value=False):
            with patch("pathlib.Path.is_file", return_value=False):
                assert is_git_repository(Path("/mock/strange_repo")) is False


def test_calculate_cost():
    """Test the calculate_cost function with different token counts and models."""
    # Test with standard model and default tier (below 200k tokens)
    cost = calculate_cost("gemini-2.5-pro-preview-03-25", 100_000, 50_000)
    # Expected: (100,000 / 1M) * 1.25 + (50,000 / 1M) * 10.00 = 0.125 + 0.5 = 0.625
    assert cost == 0.625

    # Test with standard model and higher tier (above 200k tokens)
    cost = calculate_cost("gemini-2.5-pro-preview-03-25", 250_000, 300_000)
    # Expected: (250,000 / 1M) * 2.50 + (300,000 / 1M) * 15.00 = 0.625 + 4.5 = 5.125
    assert cost == 5.125

    # Test with standard model and mixed tiers
    cost = calculate_cost("gemini-2.5-pro-preview-03-25", 150_000, 250_000)
    # Expected: (150,000 / 1M) * 1.25 + (250,000 / 1M) * 15.00 = 0.1875 + 3.75 = 3.9375
    assert cost == 3.9375

    # Test with flash model (same pricing across tiers)
    cost = calculate_cost("gemini-2.5-flash-preview-04-17", 150_000, 50_000)
    # Expected: (150,000 / 1M) * 0.15 + (50,000 / 1M) * 3.50 = 0.0225 + 0.175 = 0.1975
    assert cost == 0.1975

    # Test with unknown model
    cost = calculate_cost("unknown-model", 100_000, 50_000)
    assert cost is None


def test_format_metrics_section():
    """Test the format_metrics_section function with different metadata."""
    # Test with all metadata provided
    metadata = MagicMock()
    metadata.prompt_token_count = 1000
    metadata.candidates_token_count = 500
    metadata.total_token_count = 1500

    model = "gemini-2.5-pro-preview-03-25"

    with patch("yellhorn_mcp.server.calculate_cost") as mock_calculate_cost:
        mock_calculate_cost.return_value = 0.0175

        result = format_metrics_section(model, metadata)

        # Check that it contains all the expected sections
        assert "\n\n---\n## Completion Metrics" in result
        assert f"**Model Used**: `{model}`" in result
        assert "**Input Tokens**: 1000" in result
        assert "**Output Tokens**: 500" in result
        assert "**Total Tokens**: 1500" in result
        assert "**Estimated Cost**: $0.0175" in result

        # Check the calculate_cost was called with the right parameters
        mock_calculate_cost.assert_called_once_with(model, 1000, 500)

    # Create a custom dict-like object instead of MagicMock
    class CustomMetadata:
        def __init__(self):
            self.prompt_token_count = 2000
            self.candidates_token_count = 800
            self.total_token_count = 2800

    metadata2 = CustomMetadata()

    with patch("yellhorn_mcp.server.calculate_cost") as mock_calculate_cost:
        mock_calculate_cost.return_value = 0.035

        result = format_metrics_section(model, metadata2)

        # Check that the total is calculated
        assert "**Total Tokens**: 2800" in result

        # Check the calculate_cost was called with the right parameters
        mock_calculate_cost.assert_called_once_with(model, 2000, 800)


@pytest.mark.asyncio
async def test_create_workplan(mock_request_context, mock_genai_client):
    """Test creating a workplan."""
    # Set the mock client in the context
    mock_request_context.request_context.lifespan_context["client"] = mock_genai_client

    with patch("yellhorn_mcp.server.ensure_label_exists") as mock_ensure_label:
        with patch("yellhorn_mcp.server.run_github_command") as mock_gh:
            mock_gh.return_value = "https://github.com/user/repo/issues/123"

            with patch("asyncio.create_task") as mock_create_task:
                # Test with required title and detailed description (default codebase_reasoning="full")
                response = await create_workplan(
                    title="Feature Implementation Plan",
                    detailed_description="Create a new feature to support X",
                    ctx=mock_request_context,
                )

                # Parse response as JSON and check contents
                import json

                result = json.loads(response)
                assert result["issue_url"] == "https://github.com/user/repo/issues/123"
                assert result["issue_number"] == "123"

                mock_ensure_label.assert_called_once_with(
                    Path("/mock/repo"), "yellhorn-mcp", "Issues created by yellhorn-mcp"
                )
                mock_gh.assert_called_once()
                mock_create_task.assert_called_once()

                # Check that the GitHub issue is created with the provided title and yellhorn-mcp label
                issue_call_args = mock_gh.call_args[0]
                assert "issue" in issue_call_args[1]
                assert "create" in issue_call_args[1]
                assert "Feature Implementation Plan" in issue_call_args[1]
                assert "--label" in issue_call_args[1]
                assert "yellhorn-mcp" in issue_call_args[1]

                # Get the body argument which is '--body' followed by the content
                body_index = issue_call_args[1].index("--body") + 1
                body_content = issue_call_args[1][body_index]
                assert "# Feature Implementation Plan" in body_content
                assert "## Description" in body_content
                assert "Create a new feature to support X" in body_content
                # Verify the placeholder message for AI processing is included
                assert "Generating detailed workplan" in body_content

                # Check that the process_workplan_async task is created with the correct parameters
                args, kwargs = mock_create_task.call_args
                coroutine = args[0]
                assert coroutine.__name__ == "process_workplan_async"

                # Reset mocks for next test
                mock_ensure_label.reset_mock()
                mock_gh.reset_mock()
                mock_create_task.reset_mock()

                # Test with codebase_reasoning="none"
                response = await create_workplan(
                    title="Basic Plan",
                    detailed_description="Simple plan description",
                    ctx=mock_request_context,
                    codebase_reasoning="none",
                )

                # Parse response as JSON and check contents
                result = json.loads(response)
                assert result["issue_url"] == "https://github.com/user/repo/issues/123"
                assert result["issue_number"] == "123"

                mock_ensure_label.assert_called_once()
                mock_gh.assert_called_once()
                # Verify that no async task was created for AI processing
                mock_create_task.assert_not_called()

                # Check the body content again
                body_index = mock_gh.call_args[0][1].index("--body") + 1
                body_content = mock_gh.call_args[0][1][body_index]
                assert "# Basic Plan" in body_content
                assert "## Description" in body_content
                assert "Simple plan description" in body_content
                # Verify the placeholder message is NOT present
                assert "Generating detailed workplan" not in body_content


@pytest.mark.asyncio
async def test_run_github_command_success():
    """Test successful GitHub CLI command execution."""
    with patch("asyncio.create_subprocess_exec") as mock_exec:
        mock_process = AsyncMock()
        mock_process.communicate.return_value = (b"output", b"")
        mock_process.returncode = 0
        mock_exec.return_value = mock_process

        result = await run_github_command(Path("/mock/repo"), ["issue", "list"])

        assert result == "output"
        mock_exec.assert_called_once()

    # Ensure no coroutines are left behind
    await asyncio.sleep(0)


@pytest.mark.asyncio
async def test_ensure_label_exists():
    """Test ensuring a GitHub label exists."""
    with patch("yellhorn_mcp.server.run_github_command") as mock_gh:
        # Test with label name only
        await ensure_label_exists(Path("/mock/repo"), "test-label")
        mock_gh.assert_called_once_with(Path("/mock/repo"), ["label", "create", "test-label", "-f"])

        # Reset mock
        mock_gh.reset_mock()

        # Test with label name and description
        await ensure_label_exists(Path("/mock/repo"), "test-label", "Test label description")
        mock_gh.assert_called_once_with(
            Path("/mock/repo"),
            ["label", "create", "test-label", "-f", "--description", "Test label description"],
        )

        # Reset mock
        mock_gh.reset_mock()

        # Test with error handling (should not raise exception)
        mock_gh.side_effect = Exception("Label creation failed")
        # This should not raise an exception
        await ensure_label_exists(Path("/mock/repo"), "test-label")


@pytest.mark.asyncio
async def test_get_github_issue_body():
    """Test fetching GitHub issue body."""
    with patch("yellhorn_mcp.server.run_github_command") as mock_gh:
        # Test fetching issue content with URL
        mock_gh.return_value = '{"body": "Issue content"}'
        issue_url = "https://github.com/user/repo/issues/123"

        result = await get_github_issue_body(Path("/mock/repo"), issue_url)

        assert result == "Issue content"
        mock_gh.assert_called_once_with(
            Path("/mock/repo"), ["issue", "view", "123", "--json", "body"]
        )

        # Reset mock
        mock_gh.reset_mock()

        # Test fetching PR content with URL
        mock_gh.return_value = '{"body": "PR content"}'
        pr_url = "https://github.com/user/repo/pull/456"

        result = await get_github_issue_body(Path("/mock/repo"), pr_url)

        assert result == "PR content"
        mock_gh.assert_called_once_with(Path("/mock/repo"), ["pr", "view", "456", "--json", "body"])

        # Reset mock
        mock_gh.reset_mock()

        # Test fetching issue content with just issue number
        mock_gh.return_value = '{"body": "Issue content from number"}'
        issue_number = "789"

        result = await get_github_issue_body(Path("/mock/repo"), issue_number)

        assert result == "Issue content from number"
        mock_gh.assert_called_once_with(
            Path("/mock/repo"), ["issue", "view", "789", "--json", "body"]
        )


@pytest.mark.asyncio
async def test_get_github_pr_diff():
    """Test fetching GitHub PR diff."""
    with patch("yellhorn_mcp.server.run_github_command") as mock_gh:
        mock_gh.return_value = "diff content"
        pr_url = "https://github.com/user/repo/pull/123"

        result = await get_github_pr_diff(Path("/mock/repo"), pr_url)

        assert result == "diff content"
        mock_gh.assert_called_once_with(Path("/mock/repo"), ["pr", "diff", "123"])


@pytest.mark.asyncio
async def test_post_github_pr_review():
    """Test posting GitHub PR review."""
    with (
        patch("pathlib.Path.exists", return_value=True),
        patch("pathlib.Path.unlink") as mock_unlink,
        patch("builtins.open", create=True),
        patch("yellhorn_mcp.server.run_github_command") as mock_gh,
    ):
        mock_gh.return_value = "Review posted"
        pr_url = "https://github.com/user/repo/pull/123"

        result = await post_github_pr_review(Path("/mock/repo"), pr_url, "Review content")

        assert "Review posted successfully" in result
        mock_gh.assert_called_once()
        # Verify the PR number is extracted correctly
        args, kwargs = mock_gh.call_args
        assert "123" in args[1]
        # Verify temp file is cleaned up
        mock_unlink.assert_called_once()


@pytest.mark.asyncio
async def test_add_github_issue_comment():
    """Test adding a comment to a GitHub issue."""
    with (
        patch("builtins.open", create=True),
        patch("pathlib.Path.exists", return_value=True),
        patch("pathlib.Path.unlink") as mock_unlink,
        patch("yellhorn_mcp.server.run_github_command") as mock_gh,
    ):
        await add_github_issue_comment(Path("/mock/repo"), "123", "Comment content")

        mock_gh.assert_called_once()
        # Verify the issue number and command are correct
        args, kwargs = mock_gh.call_args
        assert args[0] == Path("/mock/repo")
        assert "issue" in args[1]
        assert "comment" in args[1]
        assert "123" in args[1]
        assert "--body-file" in args[1]
        # Verify temp file is cleaned up
        mock_unlink.assert_called_once()

        # Test with error
        mock_gh.reset_mock()
        mock_unlink.reset_mock()
        mock_gh.side_effect = Exception("Comment failed")

        with pytest.raises(YellhornMCPError, match="Failed to add comment to GitHub issue"):
            await add_github_issue_comment(Path("/mock/repo"), "123", "Comment content")


@pytest.mark.asyncio
async def test_update_github_issue():
    """Test updating a GitHub issue."""
    with (
        patch("builtins.open", create=True),
        patch("pathlib.Path.exists", return_value=True),
        patch("pathlib.Path.unlink") as mock_unlink,
        patch("yellhorn_mcp.server.run_github_command") as mock_gh,
    ):

        await update_github_issue(Path("/mock/repo"), "123", "Updated content")

        mock_gh.assert_called_once()
        # Verify temp file is cleaned up
        mock_unlink.assert_called_once()


@pytest.mark.asyncio
async def test_process_workplan_async(mock_request_context, mock_genai_client):
    """Test processing workplan asynchronously."""
    # Set the mock client in the context
    mock_request_context.request_context.lifespan_context["gemini_client"] = mock_genai_client
    mock_request_context.request_context.lifespan_context["openai_client"] = None

    with (
        patch("yellhorn_mcp.server.get_codebase_snapshot") as mock_snapshot,
        patch("yellhorn_mcp.server.format_codebase_for_prompt") as mock_format,
        patch("yellhorn_mcp.server.update_github_issue") as mock_update,
        patch("yellhorn_mcp.server.format_metrics_section") as mock_format_metrics,
    ):

        mock_snapshot.return_value = (["file1.py"], {"file1.py": "content"})
        mock_format.return_value = "Formatted codebase"
        mock_format_metrics.return_value = (
            "\n\n---\n## Completion Metrics\n*   **Model Used**: `gemini-model`"
        )

        # Set usage metadata on the response
        mock_response = mock_genai_client.aio.models.generate_content.return_value
        mock_response.usage_metadata = {
            "prompt_token_count": 1000,
            "candidates_token_count": 500,
            "total_token_count": 1500,
        }

        # Test with required parameters
        await process_workplan_async(
            Path("/mock/repo"),
            mock_genai_client,
            None,  # No OpenAI client
            "gemini-model",
            "Feature Implementation Plan",
            "123",
            mock_request_context,
            detailed_description="Create a new feature to support X",
        )

        # Check that the API was called with the right model and parameters
        mock_genai_client.aio.models.generate_content.assert_called_once()
        args, kwargs = mock_genai_client.aio.models.generate_content.call_args
        assert kwargs.get("model") == "gemini-model"

        # Check basic prompt content
        prompt_content = kwargs.get("contents", "")
        assert "<title>" in prompt_content
        assert "Feature Implementation Plan" in prompt_content
        assert "<detailed_description>" in prompt_content
        assert "Create a new feature to support X" in prompt_content

        # Check for sub-LLM guidance content
        assert "## Instructions for Workplan Structure" in prompt_content
        assert 'ALWAYS start your workplan with a "## Summary" section' in prompt_content
        assert "guide a sub-LLM that needs to understand the workplan" in prompt_content
        assert "## Implementation Steps" in prompt_content
        assert "## Technical Details" in prompt_content
        assert "## Files to Modify" in prompt_content
        assert (
            "without additional context, and structured in a way that makes it easy for an LLM"
            in prompt_content
        )

        # Check that format_metrics_section was called with the correct parameters
        mock_format_metrics.assert_called_once_with("gemini-model", mock_response.usage_metadata)

        # Check that the issue was updated with the workplan including the title and metrics
        mock_update.assert_called_once()
        args, kwargs = mock_update.call_args
        assert args[0] == Path("/mock/repo")
        assert args[1] == "123"
        assert (
            args[2]
            == "# Feature Implementation Plan\n\nMock response text\n\n---\n## Completion Metrics\n*   **Model Used**: `gemini-model`"
        )


@pytest.mark.asyncio
async def test_process_workplan_async_empty_response(mock_request_context, mock_genai_client):
    """Test processing workplan asynchronously with empty API response."""
    # Set the mock client in the context
    mock_request_context.request_context.lifespan_context["gemini_client"] = mock_genai_client
    mock_request_context.request_context.lifespan_context["openai_client"] = None

    with (
        patch("yellhorn_mcp.server.get_codebase_snapshot") as mock_snapshot,
        patch("yellhorn_mcp.server.format_codebase_for_prompt") as mock_format,
        patch("yellhorn_mcp.server.add_github_issue_comment") as mock_add_comment,
        patch("yellhorn_mcp.server.update_github_issue") as mock_update,
    ):
        mock_snapshot.return_value = (["file1.py"], {"file1.py": "content"})
        mock_format.return_value = "Formatted codebase"

        # Set empty response
        mock_response = mock_genai_client.aio.models.generate_content.return_value
        mock_response.text = ""

        # Run the function
        await process_workplan_async(
            Path("/mock/repo"),
            mock_genai_client,
            None,  # No OpenAI client
            "gemini-model",
            "Feature Implementation Plan",
            "123",
            mock_request_context,
            detailed_description="Create a new feature to support X",
        )

        # Check that add_github_issue_comment was called instead of update_github_issue
        mock_add_comment.assert_called_once()
        args, kwargs = mock_add_comment.call_args
        assert args[0] == Path("/mock/repo")
        assert args[1] == "123"
        assert "⚠️ AI workplan enhancement failed" in args[2]
        assert "empty response" in args[2]

        # Verify update_github_issue was not called
        mock_update.assert_not_called()


@pytest.mark.asyncio
async def test_process_workplan_async_error(mock_request_context, mock_genai_client):
    """Test processing workplan asynchronously with API error."""
    # Set the mock client in the context
    mock_request_context.request_context.lifespan_context["gemini_client"] = mock_genai_client
    mock_request_context.request_context.lifespan_context["openai_client"] = None

    with (
        patch("yellhorn_mcp.server.get_codebase_snapshot") as mock_snapshot,
        patch("yellhorn_mcp.server.format_codebase_for_prompt") as mock_format,
        patch("yellhorn_mcp.server.add_github_issue_comment") as mock_add_comment,
        patch("yellhorn_mcp.server.update_github_issue") as mock_update,
    ):
        mock_snapshot.return_value = (["file1.py"], {"file1.py": "content"})
        mock_format.return_value = "Formatted codebase"

        # Mock API error
        mock_genai_client.aio.models.generate_content.side_effect = Exception("API error occurred")

        # Run the function
        await process_workplan_async(
            Path("/mock/repo"),
            mock_genai_client,
            None,  # No OpenAI client
            "gemini-model",
            "Feature Implementation Plan",
            "123",
            mock_request_context,
            detailed_description="Create a new feature to support X",
        )

        # Check that add_github_issue_comment was called instead of update_github_issue
        mock_add_comment.assert_called_once()
        args, kwargs = mock_add_comment.call_args
        assert args[0] == Path("/mock/repo")
        assert args[1] == "123"
        assert "⚠️ AI workplan enhancement failed" in args[2]
        assert "API error occurred" in args[2]

        # Verify update_github_issue was not called
        mock_update.assert_not_called()


@pytest.mark.asyncio
async def test_get_workplan(mock_request_context):
    """Test getting the workplan with the required issue number."""
    with patch("yellhorn_mcp.server.get_github_issue_body") as mock_get_issue:
        mock_get_issue.return_value = "# workplan\n\n1. Implement X\n2. Test X"

        # Test getting the workplan with the required issue number
        result = await get_workplan(mock_request_context, issue_number="123")

        assert result == "# workplan\n\n1. Implement X\n2. Test X"
        mock_get_issue.assert_called_once_with(
            mock_request_context.request_context.lifespan_context["repo_path"], "123"
        )

    # Test error handling
    with patch("yellhorn_mcp.server.get_github_issue_body") as mock_get_issue:
        mock_get_issue.side_effect = Exception("Failed to get issue")

        with pytest.raises(YellhornMCPError, match="Failed to retrieve workplan"):
            await get_workplan(mock_request_context, issue_number="123")


@pytest.mark.asyncio
async def test_get_workplan_with_different_issue(mock_request_context):
    """Test getting the workplan with a different issue number."""
    with patch("yellhorn_mcp.server.get_github_issue_body") as mock_get_issue:
        mock_get_issue.return_value = "# Different workplan\n\n1. Implement Y\n2. Test Y"

        # Test with a different issue number
        result = await get_workplan(
            ctx=mock_request_context,
            issue_number="456",
        )

        assert result == "# Different workplan\n\n1. Implement Y\n2. Test Y"
        mock_get_issue.assert_called_once_with(
            mock_request_context.request_context.lifespan_context["repo_path"], "456"
        )


# This test is no longer needed because issue_number is now required


# This test is no longer needed because issue number auto-detection was removed


@pytest.mark.asyncio
async def test_judge_workplan(mock_request_context, mock_genai_client):
    """Test judging work with required issue number."""
    # Set the mock client in the context
    mock_request_context.request_context.lifespan_context["client"] = mock_genai_client

    with patch("yellhorn_mcp.server.run_git_command") as mock_run_git:
        # Mock the git rev-parse commands
        mock_run_git.side_effect = ["abc1234", "def5678"]  # base_commit_hash, head_commit_hash

        with patch("yellhorn_mcp.server.get_github_issue_body") as mock_get_issue:
            mock_get_issue.return_value = "# workplan\n\n1. Implement X\n2. Test X"

            with patch("yellhorn_mcp.server.get_git_diff") as mock_get_diff:
                mock_get_diff.return_value = "diff --git a/file.py b/file.py\n+def x(): pass"

                with patch("asyncio.create_task") as mock_create_task:
                    # Test with default refs
                    result = await judge_workplan(
                        ctx=mock_request_context,
                        issue_number="123",
                    )

                # Check the result message
                assert (
                    "Judgement task initiated comparing main (`abc1234`)..HEAD (`def5678`)"
                    in result
                )
                assert "issue #123" in result
                assert "GitHub sub-issue" in result

                # Verify the function calls
                repo_path = mock_request_context.request_context.lifespan_context["repo_path"]
                mock_get_issue.assert_called_once_with(repo_path, "123")
                mock_get_diff.assert_called_once_with(repo_path, "main", "HEAD")
                mock_create_task.assert_called_once()

                # Check process_judgement_async coroutine
                coroutine = mock_create_task.call_args[0][0]
                assert coroutine.__name__ == "process_judgement_async"

                # Reset mocks for next test
                mock_get_issue.reset_mock()
                mock_get_diff.reset_mock()
                mock_create_task.reset_mock()
                mock_run_git.reset_mock()

                # New mock values for custom refs
                mock_run_git.side_effect = [
                    "v1.0-hash",
                    "feature-hash",
                ]  # base_commit_hash, head_commit_hash

                # Test with custom refs
                result = await judge_workplan(
                    ctx=mock_request_context,
                    issue_number="123",
                    base_ref="v1.0",
                    head_ref="feature-branch",
                )

                # Check custom refs were used
                assert (
                    "Judgement task initiated comparing v1.0 (`v1.0-hash`)..feature-branch (`feature-hash`)"
                    in result
                )
                repo_path = mock_request_context.request_context.lifespan_context["repo_path"]
                mock_get_diff.assert_called_once_with(repo_path, "v1.0", "feature-branch")

    # Test error handling
    with patch("yellhorn_mcp.server.get_github_issue_body") as mock_get_issue:
        mock_get_issue.side_effect = Exception("Failed to get issue")

        with pytest.raises(YellhornMCPError, match="Failed to trigger workplan judgement"):
            await judge_workplan(ctx=mock_request_context, issue_number="123")

    # Test with invalid git ref
    with patch("yellhorn_mcp.server.run_git_command") as mock_run_git:
        # Simulate error with invalid git ref
        mock_run_git.side_effect = YellhornMCPError("Invalid git reference")

        with pytest.raises(YellhornMCPError, match="Failed to trigger workplan judgement"):
            await judge_workplan(
                ctx=mock_request_context,
                issue_number="123",
                base_ref="invalid-ref",
                head_ref="invalid-ref",
            )


@pytest.mark.asyncio
async def test_judge_workplan_with_different_issue(mock_request_context, mock_genai_client):
    """Test judging work with a different issue number."""
    # Set the mock client in the context
    mock_request_context.request_context.lifespan_context["client"] = mock_genai_client

    with patch("yellhorn_mcp.server.run_git_command") as mock_run_git:
        # Mock the git rev-parse commands
        mock_run_git.side_effect = [
            "v1.0-hash",
            "feature-hash",
        ]  # base_commit_hash, head_commit_hash

        with patch("yellhorn_mcp.server.get_github_issue_body") as mock_get_issue:
            mock_get_issue.return_value = "# Different workplan\n\n1. Implement Y\n2. Test Y"

            with patch("yellhorn_mcp.server.get_git_diff") as mock_get_diff:
                mock_get_diff.return_value = "diff --git a/file.py b/file.py\n+def x(): pass"

                with patch("asyncio.create_task") as mock_create_task:
                    # Test with a different issue number and custom refs
                    base_ref = "v1.0"
                    head_ref = "feature-branch"
                    result = await judge_workplan(
                        ctx=mock_request_context,
                        issue_number="456",
                        base_ref=base_ref,
                        head_ref=head_ref,
                    )

                    # Check the result message
                    assert (
                        f"Judgement task initiated comparing {base_ref} (`v1.0-hash`)..{head_ref} (`feature-hash`)"
                        in result
                    )
                    assert "issue #456" in result
                    assert "GitHub sub-issue" in result

                    # Verify the function calls
                    repo_path = mock_request_context.request_context.lifespan_context["repo_path"]
                    mock_get_issue.assert_called_once_with(repo_path, "456")
                    mock_get_diff.assert_called_once_with(repo_path, base_ref, head_ref)
                    mock_create_task.assert_called_once()

                    # Check process_judgement_async coroutine
                    coroutine = mock_create_task.call_args[0][0]
                    assert coroutine.__name__ == "process_judgement_async"


# This test is no longer needed because issue_number is now required


# This test is no longer needed because issue number auto-detection was removed


@pytest.mark.asyncio
async def test_get_git_diff():
    """Test getting the diff between git refs."""
    with patch("yellhorn_mcp.server.run_git_command") as mock_git:
        mock_git.return_value = "diff --git a/file.py b/file.py\n+def x(): pass"

        result = await get_git_diff(Path("/mock/repo"), "main", "feature-branch")

        assert result == "diff --git a/file.py b/file.py\n+def x(): pass"
        mock_git.assert_called_once_with(Path("/mock/repo"), ["diff", "main..feature-branch"])


@pytest.mark.asyncio
async def test_create_github_subissue():
    """Test creating a GitHub sub-issue."""
    with (
        patch("yellhorn_mcp.server.ensure_label_exists") as mock_ensure_label,
        patch("pathlib.Path.exists", return_value=True),
        patch("pathlib.Path.unlink") as mock_unlink,
        patch("builtins.open", create=True),
        patch("yellhorn_mcp.server.run_github_command") as mock_gh,
    ):
        mock_gh.return_value = "https://github.com/user/repo/issues/456"

        result = await create_github_subissue(
            Path("/mock/repo"),
            "123",
            "Judgement: main..HEAD for Workplan #123",
            "## Judgement content",
            ["yellhorn-mcp"],
        )

        assert result == "https://github.com/user/repo/issues/456"
        mock_ensure_label.assert_called_once_with(
            Path("/mock/repo"),
            "yellhorn-judgement-subissue",
            "Judgement sub-issues created by yellhorn-mcp",
        )
        mock_gh.assert_called_once()
        # Verify the correct labels were passed
        args, kwargs = mock_gh.call_args
        assert "--label" in args[1]
        index = args[1].index("--label") + 1
        assert "yellhorn-mcp,yellhorn-judgement-subissue" in args[1][index]
        # Verify temp file is cleaned up
        mock_unlink.assert_called_once()


@pytest.mark.asyncio
async def test_process_judgement_async(mock_request_context, mock_genai_client):
    """Test processing judgement asynchronously."""
    # Set the mock client in the context
    mock_request_context.request_context.lifespan_context["gemini_client"] = mock_genai_client
    mock_request_context.request_context.lifespan_context["openai_client"] = None

    with (
        patch("yellhorn_mcp.server.create_github_subissue") as mock_create_subissue,
        patch("yellhorn_mcp.server.get_codebase_snapshot") as mock_snapshot,
        patch("yellhorn_mcp.server.format_codebase_for_prompt") as mock_format,
        patch("yellhorn_mcp.server.format_metrics_section") as mock_format_metrics,
    ):
        mock_snapshot.return_value = (["file1.py"], {"file1.py": "content"})
        mock_format.return_value = "Formatted codebase"
        mock_create_subissue.return_value = "https://github.com/user/repo/issues/456"
        mock_format_metrics.return_value = (
            "\n\n---\n## Completion Metrics\n*   **Model Used**: `gemini-model`"
        )

        # Set usage metadata on the response
        mock_response = mock_genai_client.aio.models.generate_content.return_value
        mock_response.usage_metadata = {
            "prompt_token_count": 2000,
            "candidates_token_count": 800,
            "total_token_count": 2800,
        }

        workplan = "1. Implement X\n2. Test X"
        diff = "diff --git a/file.py b/file.py\n+def x(): pass"
        base_ref = "main"
        head_ref = "feature-branch"
        issue_number = "42"

        # With issue number (should create sub-issue)
        response = await process_judgement_async(
            mock_request_context.request_context.lifespan_context["repo_path"],
            mock_genai_client,
            None,  # No OpenAI client
            "gemini-model",
            workplan,
            diff,
            base_ref,
            head_ref,
            issue_number,
            mock_request_context,
        )

        # Check that the response mentions the sub-issue URL
        assert "Judgement sub-issue created: https://github.com/user/repo/issues/456" in response

        # Check that the API was called with codebase included in prompt
        mock_genai_client.aio.models.generate_content.assert_called_once()
        args, kwargs = mock_genai_client.aio.models.generate_content.call_args
        assert "Formatted codebase" in kwargs.get("contents", "")
        assert f"Base ref: {base_ref}" in kwargs.get("contents", "")
        assert f"Head ref: {head_ref}" in kwargs.get("contents", "")

        # Verify structured output instructions are present
        assert "## Judgement Summary" in kwargs.get("contents", "")
        assert "## Completed Items" in kwargs.get("contents", "")
        assert "## Missing Items" in kwargs.get("contents", "")
        assert "## Incorrect Implementation" in kwargs.get("contents", "")
        assert "## Suggested Improvements / Issues" in kwargs.get("contents", "")
        assert "## Intentional Divergence Notes" in kwargs.get("contents", "")

        # Check Markdown block unwrapping instruction
        assert "IMPORTANT: Respond *only* with the Markdown content" in kwargs.get("contents", "")

        # Check that format_metrics_section was called with the correct parameters
        mock_format_metrics.assert_called_once_with("gemini-model", mock_response.usage_metadata)

        # Check that the sub-issue was created with the right parameters
        mock_create_subissue.assert_called_once()
        args, kwargs = mock_create_subissue.call_args
        assert args[0] == Path("/mock/repo")
        assert args[1] == issue_number
        assert f"Judgement: {base_ref}..{head_ref}" in args[2]
        assert "## Comparison Metadata" in args[3]
        # Check that metrics section is included in the sub-issue body
        assert "## Completion Metrics" in args[3]
        assert args[4] == ["yellhorn-mcp"]

        # Reset mocks
        mock_genai_client.aio.models.generate_content.reset_mock()
        mock_create_subissue.reset_mock()
        mock_format_metrics.reset_mock()

        # Without issue number (should not create sub-issue)
        response = await process_judgement_async(
            mock_request_context.request_context.lifespan_context["repo_path"],
            mock_genai_client,
            None,  # No OpenAI client
            "gemini-model",
            workplan,
            diff,
            base_ref,
            head_ref,
            None,
            mock_request_context,
        )

        # Verify that metrics are included in the direct output
        assert (
            response
            == "Mock response text\n\n---\n## Completion Metrics\n*   **Model Used**: `gemini-model`"
        )
        mock_genai_client.aio.models.generate_content.assert_called_once()
        mock_format_metrics.assert_called_once()
        mock_create_subissue.assert_not_called()
