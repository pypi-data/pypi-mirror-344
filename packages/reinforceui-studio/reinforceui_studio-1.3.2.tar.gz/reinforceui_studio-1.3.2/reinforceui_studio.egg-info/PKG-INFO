Metadata-Version: 2.4
Name: reinforceui-studio
Version: 1.3.2
Summary: A GUI to simplify the configuration and monitoring of RL training processes.
Home-page: https://github.com/dvalenciar/ReinforceUI-Studio
Author: David Valencia
Author-email: support@reinforceui-studio.com
License: MIT
Project-URL: Documentation, https://docs.reinforceui-studio.com
Project-URL: Repository, https://github.com/dvalenciar/ReinforceUI-Studio
Project-URL: Tracker, https://github.com/dvalenciar/ReinforceUI-Studio/issues
Keywords: reinforcement-learning machine-learning deep-learning GUI
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Environment :: X11 Applications :: Qt
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: PyQt5~=5.15.6
Requires-Dist: matplotlib~=3.9.2
Requires-Dist: PyYAML~=6.0.1
Requires-Dist: torch~=2.4.1
Requires-Dist: numpy~=1.26.4
Requires-Dist: seaborn~=0.13.2
Requires-Dist: pandas~=2.2.2
Requires-Dist: gymnasium~=1.0.0
Requires-Dist: gymnasium[classic-control]==1.0.0
Requires-Dist: gymnasium[box2d]==1.0.0
Requires-Dist: swig~=4.3.0
Requires-Dist: dm_control~=1.0.26
Requires-Dist: opencv-python-headless~=4.10.0
Requires-Dist: imageio~=2.37.0
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: project-url
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

<p align="center">
  <a href="https://docs.reinforceui-studio.com/welcome">
    <img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/cover_RL.png" alt="ReinforceUI" width="100%">
  </a>
</p>

<h1 align="center"> ReinforceUI Studio: Reinforcement Learning Made Simple</h1>

<p align="center">
  Intuitive, Powerful, and Hassle-Free RL Training & Monitoring ‚Äì All in One Place.
</p>

<p align="center">
  <a href="https://github.com/dvalenciar/ReinforceUI-Studio/actions">
    <img src="https://img.shields.io/github/actions/workflow/status/dvalenciar/ReinforceUI-Studio/pytest.yml?label=CI&branch=main" alt="Build Status">
  </a>
  <a href="https://github.com/dvalenciar/ReinforceUI-Studio/actions">
    <img src="https://img.shields.io/github/actions/workflow/status/dvalenciar/ReinforceUI-Studio/docker-publish.yml?label=Docker&branch=main" alt="Docker Status">
  </a>
  <a href="https://github.com/dvalenciar/ReinforceUI-Studio/actions/workflows/formatting.yml">
    <img src="https://img.shields.io/github/actions/workflow/status/dvalenciar/ReinforceUI-Studio/formatting.yml?label=Formatting&branch=main" alt="Formatting Status">
  </a>
  <a href="https://docs.reinforceui-studio.com/">
    <img src="https://img.shields.io/badge/Docs-Up-blue" alt="Documentation">
  </a>
  <a href="https://opensource.org/licenses/MIT">
    <img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="License">
  </a>
  <a href="https://www.python.org/downloads/release/python-310/">
    <img src="https://img.shields.io/badge/python-3.10-blue.svg" alt="Python Version">
  </a>
  <a href="https://pypi.org/project/reinforceui-studio/">
    <img src="https://img.shields.io/pypi/v/reinforceui-studio" alt="PyPI version">
  </a>
</p>


---
‚≠êÔ∏è If you find this project useful, please consider giving it a star! It really helps!

üìö Full Documentation: <a href="https://docs.reinforceui-studio.com" target="_blank">https://docs.reinforceui-studio.com</a>

üé¨ Video Demo: [YouTube Tutorial](https://www.youtube.com/watch?v=itXyyttwZ1M)

---

## What is ReinforceUI Studio?

ReinforceUI Studio is a Python-based application designed to simplify Reinforcement Learning (RL) workflows through a beautiful, intuitive GUI.
No more memorizing commands, no more juggling extra repos ‚Äì just train, monitor, and evaluate in a few clicks!

<p align="center"> <img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/new_main_window_example.gif" width="80%"> </p>





## Quickstart
Getting started with ReinforceUI Studio is fast and easy!

### üñ•Ô∏è Install and Run Locally
The easiest way to use ReinforceUI Studio is by installing it directly from PyPI. This provides a hassle-free installation, allowing you to get started quickly with no extra configuration.

Follow these simple steps:

1. Clone the repository and install dependencies

```bash
pip install reinforceui-studio
```

2. Run the application

```bash
reinforceui-studio
```

That's it! You‚Äôre ready to start training and monitoring your Reinforcement Learning agents through an intuitive GUI.

‚úÖ Tip:
If you encounter any issues, check out the [Installation Guide](https://docs.reinforceui-studio.com/user_guides/installation) in the full documentation.

## Why you should use ReinforceUI Studio
* üöÄ Instant RL Training: Configure environments, select algorithms, set hyperparameters ‚Äì all in seconds.
* üñ•Ô∏è Real-Time Dashboard: Watch your agents learn with live performance curves and metrics.
* üß† Multi-Algorithm Support: Train and compare multiple algorithms simultaneously.
* üì¶ Full Logging: Automatically save models, plots, evaluations, videos, and training stats.
* üîß Easy Customization: Adjust hyperparameters or load optimized defaults.
* üß© Environment Support: Works with MuJoCo, OpenAI Gymnasium, and DeepMind Control Suite.
* üìä Final Comparison Plots: Auto-generate publishable comparison graphs for your reports or papers.

## Quick Overview: Single and Multi-Algorithm Training

* **Single Training**: Choose an algorithm, tweak parameters, train & visualize.

* **Multi-Training**: Select several algorithms, run them simultaneously, and compare performances side-by-side.

<table align="center">
  <tr>
    <th>Selection Window</th>
    <th>Main Window Display</th>
  </tr>
  <tr>
    <td align="center"><img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/single_selection.png" width="400"></td>
    <td align="center"><img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/single_selection_main_window.png" width="400"></td>
  </tr>
  <tr>
    <td align="center"><img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/multiple_selection.png" width="400"></td>
    <td align="center"><img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/multiple_selection_main_window.png" width="400"></td>
  </tr>
</table>

## Supported Algorithms
ReinforceUI Studio supports the following algorithms:

| Algorithm | Description |
| --- | --- |
| **CTD4** | Continuous Distributional Actor-Critic Agent with a Kalman Fusion of Multiple Critics |
| **DDPG** | Deep Deterministic Policy Gradient |
| **DQN** | Deep Q-Network |
| **PPO** | Proximal Policy Optimization |
| **SAC** | Soft Actor-Critic |
| **TD3** | Twin Delayed Deep Deterministic Policy Gradient |
| **TQC** | Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics |



## Results Examples
Below are some examples of results generated by ReinforceUI Studio, showcasing the evaluation curves along with snapshots of the policies in action.

| **Algorithm** | **Platform** | **Environment**    | **Curve**                                                       | **Video**                                                                                        |
|---------------|--------------|--------------------|-----------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
| **SAC**       | DMCS         | Walker Walk        | <img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/result_examples/SAC_walker_walk.png" width="200">        | <img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/result_examples/walker_walk.gif" width="200">       | 
| **TD3**       | MuJoCo       | HalfCheetah v5     | <img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/result_examples/TD3_HalfCheetah-v5.png" width="200">     | <img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/result_examples/HalfCheetah.gif" width="200">       |
| **CDT4**      | DMCS         | Ball in cup catch  | <img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/result_examples/CTD4_ball_in_cup_catch.png" width="200"> | <img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/result_examples/ball_in_cup_catch.gif" width="200"> | 
| **DQN**       | Gymnasium    | CartPole v1        | <img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/result_examples/DQN_CartPole-v1.png" width="200">        | <img src="https://raw.githubusercontent.com/dvalenciar/ReinforceUI-Studio/main/media_resources/result_examples/CartPole.gif" width="200">          | 


## Citation
If you find ReinforceUI Studio useful for your research or project, please kindly star this repo and cite is as follows:

```
@misc{reinforce_ui_studio_2025,
  title = { ReinforceUI Studio: Simplifying Reinforcement Learning Training and Monitoring},
  author = {David Valencia Redrovan},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/dvalenciar/ReinforceUI-Studio.}
}
```


## Why Star ‚≠ê this Repository?
Your support helps the project grow!
If you like ReinforceUI Studio, please star ‚≠ê this repository and share it with friends, colleagues, and the RL community!
Together, we can make Reinforcement Learning accessible to everyone!

## License
ReinforceUI Studio is licensed under the MIT License. You are free to use, modify, and distribute this software, 
provided that the original copyright notice and license are included in any copies or substantial portions of the software.


### Acknowledgements
This project was inspired by the CARES Reinforcement Learning Package from the University of Auckland 
