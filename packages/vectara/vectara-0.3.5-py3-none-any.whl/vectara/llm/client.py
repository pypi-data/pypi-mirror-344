# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.chat_completion_request_message import ChatCompletionRequestMessage
from ..types.create_chat_completion_response import CreateChatCompletionResponse
from .raw_client import AsyncRawLlmClient, RawLlmClient

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class LlmClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawLlmClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawLlmClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawLlmClient
        """
        return self._raw_client

    def chat_completion(
        self,
        *,
        model: str,
        messages: typing.Sequence[ChatCompletionRequestMessage],
        request_timeout: typing.Optional[int] = None,
        request_timeout_millis: typing.Optional[int] = None,
        stream: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreateChatCompletionResponse:
        """
        OpenAI-compatible endpoint for chat completions. Creates a response for the given chat conversation.
        The chat completion API allows you to chat with Vectara's language models in a way that's compatible with OpenAI's specification.
        This makes it easy to integrate with applications already designed for OpenAI's API.

        Parameters
        ----------
        model : str
            The ID of the model to use. This field is required.

        messages : typing.Sequence[ChatCompletionRequestMessage]
            An ordered array of messages that represent the full context of the conversation to date. Each message includes a `role` and `content`.

        request_timeout : typing.Optional[int]
            The API will make a best effort to complete the request in the specified seconds or time out.

        request_timeout_millis : typing.Optional[int]
            The API will make a best effort to complete the request in the specified milliseconds or time out.

        stream : typing.Optional[bool]
            Optional. When set to `true`, the API streams partial message deltas as they become available, similar to ChatGPT's streaming mode.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreateChatCompletionResponse
            A chat completion

        Examples
        --------
        from vectara import Vectara
        from vectara import ChatCompletionRequestMessage
        client = Vectara(api_key="YOUR_API_KEY", client_id="YOUR_CLIENT_ID", client_secret="YOUR_CLIENT_SECRET", )
        client.llm.chat_completion(model='model', messages=[ChatCompletionRequestMessage(role='role', content='content', )], )
        """
        response = self._raw_client.chat_completion(
            model=model,
            messages=messages,
            request_timeout=request_timeout,
            request_timeout_millis=request_timeout_millis,
            stream=stream,
            request_options=request_options,
        )
        return response.data


class AsyncLlmClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawLlmClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawLlmClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawLlmClient
        """
        return self._raw_client

    async def chat_completion(
        self,
        *,
        model: str,
        messages: typing.Sequence[ChatCompletionRequestMessage],
        request_timeout: typing.Optional[int] = None,
        request_timeout_millis: typing.Optional[int] = None,
        stream: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreateChatCompletionResponse:
        """
        OpenAI-compatible endpoint for chat completions. Creates a response for the given chat conversation.
        The chat completion API allows you to chat with Vectara's language models in a way that's compatible with OpenAI's specification.
        This makes it easy to integrate with applications already designed for OpenAI's API.

        Parameters
        ----------
        model : str
            The ID of the model to use. This field is required.

        messages : typing.Sequence[ChatCompletionRequestMessage]
            An ordered array of messages that represent the full context of the conversation to date. Each message includes a `role` and `content`.

        request_timeout : typing.Optional[int]
            The API will make a best effort to complete the request in the specified seconds or time out.

        request_timeout_millis : typing.Optional[int]
            The API will make a best effort to complete the request in the specified milliseconds or time out.

        stream : typing.Optional[bool]
            Optional. When set to `true`, the API streams partial message deltas as they become available, similar to ChatGPT's streaming mode.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreateChatCompletionResponse
            A chat completion

        Examples
        --------
        from vectara import AsyncVectara
        from vectara import ChatCompletionRequestMessage
        import asyncio
        client = AsyncVectara(api_key="YOUR_API_KEY", client_id="YOUR_CLIENT_ID", client_secret="YOUR_CLIENT_SECRET", )
        async def main() -> None:
            await client.llm.chat_completion(model='model', messages=[ChatCompletionRequestMessage(role='role', content='content', )], )
        asyncio.run(main())
        """
        response = await self._raw_client.chat_completion(
            model=model,
            messages=messages,
            request_timeout=request_timeout,
            request_timeout_millis=request_timeout_millis,
            stream=stream,
            request_options=request_options,
        )
        return response.data
