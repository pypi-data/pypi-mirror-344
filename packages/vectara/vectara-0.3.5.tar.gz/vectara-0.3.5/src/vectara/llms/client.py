# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.pagination import AsyncPager, SyncPager
from ..core.request_options import RequestOptions
from ..types.llm import Llm
from ..types.remote_auth import RemoteAuth
from .raw_client import AsyncRawLlmsClient, RawLlmsClient

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class LlmsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawLlmsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawLlmsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawLlmsClient
        """
        return self._raw_client

    def list(
        self,
        *,
        filter: typing.Optional[str] = None,
        limit: typing.Optional[int] = None,
        page_key: typing.Optional[str] = None,
        request_timeout: typing.Optional[int] = None,
        request_timeout_millis: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> SyncPager[Llm]:
        """
        List LLMs that can be used with query and chat endpoints. The LLM is not directly specified in a query,
        but instead a `generation_preset_name` is used. The `generation_preset_name` property in generation parameters
        can be found as the `name` property on the Generations Presets retrieved from `/v2/generation_presets`.

        Parameters
        ----------
        filter : typing.Optional[str]
            A regular expression to match names and descriptions of the LLMs.

        limit : typing.Optional[int]
            The maximum number of results to return in the list.

        page_key : typing.Optional[str]
            Used to retrieve the next page of LLMs after the limit has been reached.
            This parameter is not needed for the first page of results.

        request_timeout : typing.Optional[int]
            The API will make a best effort to complete the request in the specified seconds or time out.

        request_timeout_millis : typing.Optional[int]
            The API will make a best effort to complete the request in the specified milliseconds or time out.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        SyncPager[Llm]
            List of LLMs.

        Examples
        --------
        from vectara import Vectara
        client = Vectara(api_key="YOUR_API_KEY", client_id="YOUR_CLIENT_ID", client_secret="YOUR_CLIENT_SECRET", )
        response = client.llms.list()
        for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        for page in response.iter_pages():
            yield page
        """
        response = self._raw_client.list(
            filter=filter,
            limit=limit,
            page_key=page_key,
            request_timeout=request_timeout,
            request_timeout_millis=request_timeout_millis,
            request_options=request_options,
        )
        return response.data

    def create(
        self,
        *,
        name: str,
        model: str,
        uri: str,
        request_timeout: typing.Optional[int] = None,
        request_timeout_millis: typing.Optional[int] = None,
        description: typing.Optional[str] = OMIT,
        auth: typing.Optional[RemoteAuth] = OMIT,
        headers: typing.Optional[typing.Dict[str, str]] = OMIT,
        test_model_parameters: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Llm:
        """
        Create a new LLM for use with query and chat endpoints

        Parameters
        ----------
        name : str
            Name to reference the LLM.  This will be used in other endpoints (like query) when using this LLM.
            If this name conflicts with a global LLM (a LLM that is precofnigured with the Vectara platform),
            then it will override that LLM for all usages.

        model : str
            The model name to use with the API (e.g. gpt-4, claude-2, etc). This is used in the API request to the remote LLM provider.

        uri : str
            The URI endpoint for the API (can be OpenAI or any compatible API endpoint)

        request_timeout : typing.Optional[int]
            The API will make a best effort to complete the request in the specified seconds or time out.

        request_timeout_millis : typing.Optional[int]
            The API will make a best effort to complete the request in the specified milliseconds or time out.

        description : typing.Optional[str]
            Description of the LLM.

        auth : typing.Optional[RemoteAuth]

        headers : typing.Optional[typing.Dict[str, str]]
            Additional HTTP headers to include with requests to the LLM API.

        test_model_parameters : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional parameters that are required for the LLM during the test call.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Llm
            The LLM has been created

        Examples
        --------
        from vectara import Vectara
        client = Vectara(api_key="YOUR_API_KEY", client_id="YOUR_CLIENT_ID", client_secret="YOUR_CLIENT_SECRET", )
        client.llms.create(name='name', model='model', uri='uri', )
        """
        response = self._raw_client.create(
            name=name,
            model=model,
            uri=uri,
            request_timeout=request_timeout,
            request_timeout_millis=request_timeout_millis,
            description=description,
            auth=auth,
            headers=headers,
            test_model_parameters=test_model_parameters,
            request_options=request_options,
        )
        return response.data

    def get(
        self,
        llm_id: str,
        *,
        request_timeout: typing.Optional[int] = None,
        request_timeout_millis: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Llm:
        """
        Get details about a specific LLM.

        Parameters
        ----------
        llm_id : str
            The name of the LLM to retrieve.

        request_timeout : typing.Optional[int]
            The API will make a best effort to complete the request in the specified seconds or time out.

        request_timeout_millis : typing.Optional[int]
            The API will make a best effort to complete the request in the specified milliseconds or time out.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Llm
            The LLM details.

        Examples
        --------
        from vectara import Vectara
        client = Vectara(api_key="YOUR_API_KEY", client_id="YOUR_CLIENT_ID", client_secret="YOUR_CLIENT_SECRET", )
        client.llms.get(llm_id='llm_id', )
        """
        response = self._raw_client.get(
            llm_id,
            request_timeout=request_timeout,
            request_timeout_millis=request_timeout_millis,
            request_options=request_options,
        )
        return response.data

    def delete(
        self,
        llm_id: str,
        *,
        request_timeout: typing.Optional[int] = None,
        request_timeout_millis: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> None:
        """
        Delete a custom LLM connection. Built-in LLMs cannot be deleted.

        Parameters
        ----------
        llm_id : str
            The name of the LLM to delete.

        request_timeout : typing.Optional[int]
            The API will make a best effort to complete the request in the specified seconds or time out.

        request_timeout_millis : typing.Optional[int]
            The API will make a best effort to complete the request in the specified milliseconds or time out.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from vectara import Vectara
        client = Vectara(api_key="YOUR_API_KEY", client_id="YOUR_CLIENT_ID", client_secret="YOUR_CLIENT_SECRET", )
        client.llms.delete(llm_id='llm_id', )
        """
        response = self._raw_client.delete(
            llm_id,
            request_timeout=request_timeout,
            request_timeout_millis=request_timeout_millis,
            request_options=request_options,
        )
        return response.data


class AsyncLlmsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawLlmsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawLlmsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawLlmsClient
        """
        return self._raw_client

    async def list(
        self,
        *,
        filter: typing.Optional[str] = None,
        limit: typing.Optional[int] = None,
        page_key: typing.Optional[str] = None,
        request_timeout: typing.Optional[int] = None,
        request_timeout_millis: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncPager[Llm]:
        """
        List LLMs that can be used with query and chat endpoints. The LLM is not directly specified in a query,
        but instead a `generation_preset_name` is used. The `generation_preset_name` property in generation parameters
        can be found as the `name` property on the Generations Presets retrieved from `/v2/generation_presets`.

        Parameters
        ----------
        filter : typing.Optional[str]
            A regular expression to match names and descriptions of the LLMs.

        limit : typing.Optional[int]
            The maximum number of results to return in the list.

        page_key : typing.Optional[str]
            Used to retrieve the next page of LLMs after the limit has been reached.
            This parameter is not needed for the first page of results.

        request_timeout : typing.Optional[int]
            The API will make a best effort to complete the request in the specified seconds or time out.

        request_timeout_millis : typing.Optional[int]
            The API will make a best effort to complete the request in the specified milliseconds or time out.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncPager[Llm]
            List of LLMs.

        Examples
        --------
        from vectara import AsyncVectara
        import asyncio
        client = AsyncVectara(api_key="YOUR_API_KEY", client_id="YOUR_CLIENT_ID", client_secret="YOUR_CLIENT_SECRET", )
        async def main() -> None:
            response = await client.llms.list()
            async for item in response:
                yield item

            # alternatively, you can paginate page-by-page
            async for page in response.iter_pages():
                yield page
        asyncio.run(main())
        """
        response = await self._raw_client.list(
            filter=filter,
            limit=limit,
            page_key=page_key,
            request_timeout=request_timeout,
            request_timeout_millis=request_timeout_millis,
            request_options=request_options,
        )
        return response.data

    async def create(
        self,
        *,
        name: str,
        model: str,
        uri: str,
        request_timeout: typing.Optional[int] = None,
        request_timeout_millis: typing.Optional[int] = None,
        description: typing.Optional[str] = OMIT,
        auth: typing.Optional[RemoteAuth] = OMIT,
        headers: typing.Optional[typing.Dict[str, str]] = OMIT,
        test_model_parameters: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Llm:
        """
        Create a new LLM for use with query and chat endpoints

        Parameters
        ----------
        name : str
            Name to reference the LLM.  This will be used in other endpoints (like query) when using this LLM.
            If this name conflicts with a global LLM (a LLM that is precofnigured with the Vectara platform),
            then it will override that LLM for all usages.

        model : str
            The model name to use with the API (e.g. gpt-4, claude-2, etc). This is used in the API request to the remote LLM provider.

        uri : str
            The URI endpoint for the API (can be OpenAI or any compatible API endpoint)

        request_timeout : typing.Optional[int]
            The API will make a best effort to complete the request in the specified seconds or time out.

        request_timeout_millis : typing.Optional[int]
            The API will make a best effort to complete the request in the specified milliseconds or time out.

        description : typing.Optional[str]
            Description of the LLM.

        auth : typing.Optional[RemoteAuth]

        headers : typing.Optional[typing.Dict[str, str]]
            Additional HTTP headers to include with requests to the LLM API.

        test_model_parameters : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional parameters that are required for the LLM during the test call.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Llm
            The LLM has been created

        Examples
        --------
        from vectara import AsyncVectara
        import asyncio
        client = AsyncVectara(api_key="YOUR_API_KEY", client_id="YOUR_CLIENT_ID", client_secret="YOUR_CLIENT_SECRET", )
        async def main() -> None:
            await client.llms.create(name='name', model='model', uri='uri', )
        asyncio.run(main())
        """
        response = await self._raw_client.create(
            name=name,
            model=model,
            uri=uri,
            request_timeout=request_timeout,
            request_timeout_millis=request_timeout_millis,
            description=description,
            auth=auth,
            headers=headers,
            test_model_parameters=test_model_parameters,
            request_options=request_options,
        )
        return response.data

    async def get(
        self,
        llm_id: str,
        *,
        request_timeout: typing.Optional[int] = None,
        request_timeout_millis: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Llm:
        """
        Get details about a specific LLM.

        Parameters
        ----------
        llm_id : str
            The name of the LLM to retrieve.

        request_timeout : typing.Optional[int]
            The API will make a best effort to complete the request in the specified seconds or time out.

        request_timeout_millis : typing.Optional[int]
            The API will make a best effort to complete the request in the specified milliseconds or time out.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Llm
            The LLM details.

        Examples
        --------
        from vectara import AsyncVectara
        import asyncio
        client = AsyncVectara(api_key="YOUR_API_KEY", client_id="YOUR_CLIENT_ID", client_secret="YOUR_CLIENT_SECRET", )
        async def main() -> None:
            await client.llms.get(llm_id='llm_id', )
        asyncio.run(main())
        """
        response = await self._raw_client.get(
            llm_id,
            request_timeout=request_timeout,
            request_timeout_millis=request_timeout_millis,
            request_options=request_options,
        )
        return response.data

    async def delete(
        self,
        llm_id: str,
        *,
        request_timeout: typing.Optional[int] = None,
        request_timeout_millis: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> None:
        """
        Delete a custom LLM connection. Built-in LLMs cannot be deleted.

        Parameters
        ----------
        llm_id : str
            The name of the LLM to delete.

        request_timeout : typing.Optional[int]
            The API will make a best effort to complete the request in the specified seconds or time out.

        request_timeout_millis : typing.Optional[int]
            The API will make a best effort to complete the request in the specified milliseconds or time out.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from vectara import AsyncVectara
        import asyncio
        client = AsyncVectara(api_key="YOUR_API_KEY", client_id="YOUR_CLIENT_ID", client_secret="YOUR_CLIENT_SECRET", )
        async def main() -> None:
            await client.llms.delete(llm_id='llm_id', )
        asyncio.run(main())
        """
        response = await self._raw_client.delete(
            llm_id,
            request_timeout=request_timeout,
            request_timeout_millis=request_timeout_millis,
            request_options=request_options,
        )
        return response.data
