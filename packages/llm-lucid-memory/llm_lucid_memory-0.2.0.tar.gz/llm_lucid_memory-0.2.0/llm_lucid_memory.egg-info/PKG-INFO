Metadata-Version: 2.4
Name: llm-lucid-memory
Version: 0.2.0
Summary: Lucid Memory - Modular reasoning brain for small LLMs
Home-page: https://github.com/benschneider/llm-lucid-memory
Author: Ben Schneider
Author-email: benh.schneider@gmail.com
Project-URL: Source, https://github.com/benschneider/llm-lucid-memory
Project-URL: Issue Tracker, https://github.com/benschneider/llm-lucid-memory/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Developers
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: fastapi
Requires-Dist: uvicorn
Requires-Dist: requests
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: project-url
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

[![License](https://img.shields.io/github/license/benschneider/llm-lucid-memory)](LICENSE)

[![Advanced Reasoning Project](https://img.shields.io/badge/advanced%20reasoning-in%20progress-blue)]()

---

## 📚 Table of Contents
- [Why Lucid Memory](#-why-lucid-memory)
- [Concept Overview](#-concept-overview)
- [Core Reasoning Flow](#-core-reasoning-flow)
- [Quick Start](#-quick-start)
- [Example Usage](#-example-usage)
- [What's Coming Next](#-whats-coming-next)
- [Advanced Research Directions](#-advanced-research-directions)
- [License](#-license)
- [Vision](#-vision)
- [How to Use with Local LLMs](#-how-to-use-with-local-llms-ollama-lmstudio-etc)
- [Current Status and Next Steps](#-current-status-and-next-steps)
- [Project Structure](#-project-structure)

---

# 🧠 llm-lucid-memory

**Lucid Memory** is an open-source project aiming to enable small and medium LLMs to **reason beyond their context windows** — through modular memory digestion, structured storage, reflective retrieval, and chain-of-draft reasoning.

> **Imagine:** Your model thinking more like a brain, not just predicting the next token.

---

## 🌟 Why Lucid Memory?

- **Digest** knowledge offline into modular, logic-rich memory nodes
- **Store** memories flexibly (searchable by tags, keywords, and logic paths)
- **Reflectively retrieve** only relevant memories for a question
- **Draft answers** based on structured reasoning, not blind retrieval
- **Unlock huge knowledge bases** even with limited context models

---

## 📚 Concept Overview

**The Problem:**  
- Small LLMs can't fit large codebases or document sets.
- Existing RAG (retrieval augmented generation) just crams context into prompts.
- LLMs need *structured*, *logical*, *reflective* memory systems.

**The Solution:**  
Lucid Memory introduces a lightweight brain architecture for LLMs:
- Preprocess (digest) large information offline.
- Save modular memories with summaries and reasoning paths.
- Reflectively retrieve and reason using only what matters.

---

## 🔥 Core Reasoning Flow

```plaintext

[DIGEST PHASE - offline]
Raw Knowledge -> Digestor -> MemoryNode -> MemoryGraph (saved)

[QUERY PHASE - online]
User Question -> ReflectiveRetriever -> Select Relevant Memories -> ChainOfDraftEngine -> Monte Carlo Chain Voting -> Logical Answer

```

## 🧠 Growing Lucid Memory: Recursive Understanding and Connection Graph

We are designing Lucid Memory to not just store facts but to **grow understanding recursively**:

1. **High-Level Overview**  
   Every raw text or document is first digested into a summary, key concepts, and open follow-up questions.

2. **Recursive Exploration**  
   Follow-up questions are then digested individually to deepen understanding where needed, expanding the memory tree.

3. **Structured Memory Growth**  
   Each new memory is stored as a `MemoryNode`, annotated with relationships, follow-up links, and key topics.

4. **Embedding and Connection Graph**  
   Memory summaries and concepts are embedded into vector space.  
   Connections between memories are formed based on semantic similarity and logical dependencies, resulting in a dynamic, growing memory graph.

---


## 🧪 Quick Start

Install dependencies:
```bash
pip install -r requirements.txt
```

Start a UI to configure and start a Proxy Server for LLM interaction:
```bash 
python -m lucid_memory.gui
```

### For development:

Run tests: 
```bash 
pytest 
```

Run a simple ingestion demo: 
```bash 
python -m examples.simple_digest_demo ```
```

## 🧠 Example Usage

Question:

“How does the server start and accept secure connections?”

Memory Retrieval:
	- Memory 1: Start server (open socket, bind port, accept requests)
	- Memory 2: Handle TLS (perform handshake, secure channel)

Drafted Answer:
	- Open socket
	- Bind port
	- Accept connection
	- Initiate TLS handshake
	- Proceed with secured HTTP handling

## 🧠 Advanced Research Directions

### 🧠 Future Optimization: Monte Carlo Chain Voting for Memory Refinement

Inspired by research like ["More Reliable Code Generation via Monte Carlo Tree Search"](https://openreview.net/pdf?id=xoXn62FzD0) from MIT (summarized [here](https://news.mit.edu/2025/making-ai-generated-code-more-accurate-0418)), we plan to extend Lucid Memory with a **Monte Carlo Chain Voting** mechanism.

In the future, Lucid Memory will leverage Monte Carlo Chain Voting (MCCV) not just for reasoning — but also for optimizing the memory graph itself.

Instead of trusting the first generated reasoning path, the system will:
- Generate multiple reasoning chains using the memory graph.
- Score chains based on logical consistency and alignment with known memories.
- Identify and refine weak, redundant, or inconsistent memory nodes.
- Strengthen useful and frequently used paths over time.

This approach allows the memory system to grow smarter naturally, even without direct weight tuning, by continuously improving its structure through reflective evaluation.

Stay tuned as we integrate these innovations into future versions! 🚀


## 🛣️ v0.2.0 Roadmap

**Upcoming Features and Improvements:**

| Feature | Status |
|:---|:---|
| ChainOfDraftEngine for multi-path logical drafting | 🔜 In Progress |
| Monte Carlo Chain Voting for output reliability | 🔜 Researching |
| Memory digest via direct LLM connection (Ollama/LMStudio) | ✅ Part 1 Done |
| Proxy Server separation for chat handling only | ✅ Done |
| Smart GUI for managing memory, loading contexts, and proxy control | ✅ Done |
| Server health check before digestion | 🔜 Planned |
| Batch context ingestion support | 🔜 Planned |
| Memory browsing improvements (filter/search) | 🔜 Planned |
| Config-driven setup via `proxy_config.json` | ✅ Done |
| MemoryNode versioning                 | 🔜 Future  |
| Graph-based retrieval paths           | 🔜 Future  |
| Sleep-time memory growth (dream ingestion) | 🔜 Future  |
| Fine-tuning LLMs for memory reasoning  | 🔜 Future  |


Stay tuned — next update will bring Lucid Memory closer to fully modular, self-optimizing small-brain architecture! 🧠🚀

## 📜 License

Apache 2.0 — free for commercial and research use with attribution.

## ✨ Vision

We are building a modular reasoning brain for LLMs:
    - Digest structured memories
    - Reflectively retrieve knowledge
    - Reason flexibly beyond context limits
    - Grow smarter over time

Helping small models think big - the way real minds do. 🚀

## 🔌 How to Use with Local LLMs (Ollama, LMStudio, etc.)

- Start your local LLM server (e.g., Ollama, LMStudio)
- Edit `lucid_memory/proxy_config.json` to configure:
  - LLM Backend URL (e.g., http://localhost:11434/v1/chat/completions)
  - LLM Model Name (e.g., mistral, phi3)
- Launch the Lucid Memory GUI via:
  ```bash
  python -m lucid_memory.gui
  ```
- Start the Proxy Server using the GUI
- Enjoy memory-enhanced, chain-of-draft smart reasoning!

## 🚧 Current Status and Next Steps

Lucid Memory is actively under development.

**Current Highlights:**
- Core MemoryNode and MemoryGraph structures are functional and fully tested.
- ReflectiveRetriever operational for smart memory selection.
- Proxy server connects to local LLMs like Ollama and LMStudio, injecting smart memory context.
- First end-to-end smart chain-of-draft flow tested successfully.

This approach aims to create an AI memory system that:
- Reflects real understanding, not just surface facts.
- Grows organically through recursive thinking.
- Can retrieve, reason, and refine knowledge dynamically over time.

**Next Major Implementations:**
- Launch ChainOfDraftEngine to structure logical answer chaining.
- Add Monte Carlo Chain Voting to improve answer reliability and reduce hallucinations.
- Enhance start.py UI to allow manual preprocessing of knowledge (digesting custom text/code snippets).
- Develop a lightweight fine-tuning dataset for memory-enhanced reasoning.

Stay tuned — the brain is growing! 🧠🚀


## 📦 Project Structure

```plaintext
llm-lucid-memory/
├── README.md            # Project overview
├── LICENSE              # Apache 2.0 License
├── lucid_memory/        # Core modules
│   ├── __init__.py
│   ├── digestor.py       # Digest raw input into MemoryNodes
│   ├── memory_node.py    # Single knowledge atoms
│   ├── memory_graph.py   # In-memory brain managing MemoryNodes
│   ├── retriever.py      # ReflectiveRetriever (smart retrieval)
│   ├── proxy_server.py   # Proxy server bridging app ↔ smart LLM context
│   └── gui.py             # GUI launcher for proxy server, memory loading, and configuration
├── examples/            # Demos and pipelines
│   ├── simple_digest_demo.py
│   ├── full_pipeline_demo.py
│   └── test_client.py
├── tests/               # Test suites for all modules
│   ├── test_digestor.py
│   ├── test_memory_graph.py
│   ├── test_memory_node.py
│   ├── test_retriever.py
│   └── (more planned)
├── requirements.txt     # Lightweight dependencies
└── setup.py             # Optional pip packaging
```
