{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Classification with PyTorch Neural Network\n\nAn example of how to cooperate imbens with deep learning frameworks like PyTorch.\nIn this example, we first define a simple MLP classifier using PyTorch, and pack \nit into a scikit-learn-style wrapper class `TorchMLPClassifier`. We then use it as \nthe base estimator of :class:`imbens.ensemble.SelfPacedEnsembleClassifier` and \ntrain the ensemble on a toy imbalanced dataset.\n\nThis example uses:\n    \n    - :class:`imbens.ensemble.SelfPacedEnsembleClassifier`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Zhining Liu <zhining.liu@outlook.com>\n# License: MIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\n# Import imbalanced-ensemble\nimport imbens\n\n# Import pytorch and numpy\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n# Import utilities\nimport sklearn\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.validation import validate_data\nfrom imbens.datasets import make_imbalance\n\n# Import plot utilities\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nRANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch MLPClassifier\n**Define a simple 3-layer perceptron with PyTorch.**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Wrap the MLP into a scikit-learn-style ``TorchMLPClassifier`` class.**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class TorchMLPClassifier(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        output_size,\n        learning_rate=0.01,\n        num_epochs=50,\n        batch_size=32,\n    ):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.model = MLP(input_size, hidden_size, output_size)\n\n    def _validate_input(self, X, y):\n        X, y = validate_data(\n            self,\n            X,\n            y,\n            accept_sparse=[\"csr\", \"csc\"],\n            multi_output=True,\n            dtype=(np.float64, np.float32),\n            reset=True,\n        )\n        self.classes_ = sklearn.utils.multiclass.unique_labels(y)\n        return X, y\n\n    def fit(self, X, y):\n\n        X, y = self._validate_input(X, y)\n\n        # Convert data to PyTorch tensors\n        X_tensor = torch.tensor(X, dtype=torch.float32)\n        y_tensor = torch.tensor(y, dtype=torch.long)\n\n        # Define loss function and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n\n        # Train the model\n        for epoch in range(self.num_epochs):\n            for i in range(0, len(X), self.batch_size):\n                # Forward pass\n                outputs = self.model(X_tensor[i : i + self.batch_size])\n\n                # Compute loss\n                loss = criterion(outputs, y_tensor[i : i + self.batch_size])\n\n                # Backward and optimize\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n    def predict(self, X):\n        # Convert data to PyTorch tensor\n        X_tensor = torch.tensor(X, dtype=torch.float32)\n\n        # Forward pass and get predictions\n        outputs = self.model(X_tensor)\n        _, predicted = torch.max(outputs.data, 1)\n\n        # Convert predictions to numpy array and return\n        return predicted.numpy()\n\n    def predict_proba(self, X):\n        # Convert data to PyTorch tensor\n        X_tensor = torch.tensor(X, dtype=torch.float32)\n\n        # Forward pass and get softmax probabilities\n        outputs = self.model(X_tensor)\n        softmax = nn.Softmax(dim=1)\n        probabilities = softmax(outputs).detach().numpy()\n\n        # Return probabilities\n        return probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification and Visualization\n**Prepare the class-imbalanced toy dataset.**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# imbalanced moons dataset\ndistribution = {0: 100, 1: 50}\nX, y = make_moons(200, noise=0.2, random_state=RANDOM_STATE)\nimb_moons_dataset = make_imbalance(\n    X, y, sampling_strategy=distribution, random_state=RANDOM_STATE\n)\nclasses = sklearn.utils.multiclass.unique_labels(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Use the ``TorchMLPClassifier`` as the ensemble base estimator.**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch_spe = imbens.ensemble.SelfPacedEnsembleClassifier(\n    estimator=TorchMLPClassifier(\n        input_size=X.shape[1], hidden_size=64, output_size=classes.shape[0]\n    ),\n    n_estimators=10,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Visualize function.**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_classification_result(dataset, clf, **axset_kwargs):\n    h = 0.01  # step size in the mesh\n    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n\n    # Normalize and split the dataset\n    X, y = dataset\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.4, random_state=42\n    )\n\n    # Prepare the meshgrid for plotting\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    clf.fit(X_train, y_train)\n    score = sklearn.metrics.average_precision_score(y_test, clf.predict(X_test))\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    if hasattr(clf, \"decision_function\"):\n        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    else:\n        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    ax = plt.gca()\n    ax.imshow(\n        -Z, extent=(xx.min(), xx.max(), yy.max(), yy.min()), cmap=\"bwr\", alpha=0.8\n    )\n\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n    # Plot the testing points\n    ax.scatter(\n        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, edgecolors=\"k\", alpha=0.6\n    )\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.text(\n        0.95,\n        0.06,\n        (\"%.2f\" % score).lstrip(\"0\"),\n        size=15,\n        bbox=dict(boxstyle=\"round\", alpha=0.8, facecolor=\"white\"),\n        transform=ax.transAxes,\n        horizontalalignment=\"right\",\n    )\n    ax.set(**axset_kwargs)\n    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Visualize the classification result.**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = plot_classification_result(\n    imb_moons_dataset, torch_spe, title=\"SPE with PyTorch MLP base classifier\"\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}