# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
from .. import _utilities

__all__ = [
    'QueueScalingPolicyArgs',
    'QueueSparkDriverArgs',
    'SparkJobDependentPackageArgs',
    'SparkJobDependentPackagePackageArgs',
    'SqlJobConfArgs',
    'TableColumnArgs',
]

@pulumi.input_type
class QueueScalingPolicyArgs:
    def __init__(__self__, *,
                 impact_start_time: pulumi.Input[str],
                 impact_stop_time: pulumi.Input[str],
                 max_cu: pulumi.Input[int],
                 min_cu: pulumi.Input[int],
                 priority: pulumi.Input[int]):
        """
        :param pulumi.Input[str] impact_start_time: Specifies the effective time of the queue scaling policy.
               The value can be set only by hour.
        :param pulumi.Input[str] impact_stop_time: Specifies the expiration time of the queue scaling policy.
               The value can be set only by hour.
        :param pulumi.Input[int] max_cu: Specifies the maximum number of CUs allowed by the scaling policy.
               The number must be a multiple of `4`.
        :param pulumi.Input[int] min_cu: Specifies the minimum number of CUs allowed by the scaling policy.
               The number must be a multiple of `4`.
        :param pulumi.Input[int] priority: Specifies the priority of the queue scaling policy.
               The valid value ranges from `1` to `100`. The larger value means the higher priority.
        """
        pulumi.set(__self__, "impact_start_time", impact_start_time)
        pulumi.set(__self__, "impact_stop_time", impact_stop_time)
        pulumi.set(__self__, "max_cu", max_cu)
        pulumi.set(__self__, "min_cu", min_cu)
        pulumi.set(__self__, "priority", priority)

    @property
    @pulumi.getter(name="impactStartTime")
    def impact_start_time(self) -> pulumi.Input[str]:
        """
        Specifies the effective time of the queue scaling policy.
        The value can be set only by hour.
        """
        return pulumi.get(self, "impact_start_time")

    @impact_start_time.setter
    def impact_start_time(self, value: pulumi.Input[str]):
        pulumi.set(self, "impact_start_time", value)

    @property
    @pulumi.getter(name="impactStopTime")
    def impact_stop_time(self) -> pulumi.Input[str]:
        """
        Specifies the expiration time of the queue scaling policy.
        The value can be set only by hour.
        """
        return pulumi.get(self, "impact_stop_time")

    @impact_stop_time.setter
    def impact_stop_time(self, value: pulumi.Input[str]):
        pulumi.set(self, "impact_stop_time", value)

    @property
    @pulumi.getter(name="maxCu")
    def max_cu(self) -> pulumi.Input[int]:
        """
        Specifies the maximum number of CUs allowed by the scaling policy.
        The number must be a multiple of `4`.
        """
        return pulumi.get(self, "max_cu")

    @max_cu.setter
    def max_cu(self, value: pulumi.Input[int]):
        pulumi.set(self, "max_cu", value)

    @property
    @pulumi.getter(name="minCu")
    def min_cu(self) -> pulumi.Input[int]:
        """
        Specifies the minimum number of CUs allowed by the scaling policy.
        The number must be a multiple of `4`.
        """
        return pulumi.get(self, "min_cu")

    @min_cu.setter
    def min_cu(self, value: pulumi.Input[int]):
        pulumi.set(self, "min_cu", value)

    @property
    @pulumi.getter
    def priority(self) -> pulumi.Input[int]:
        """
        Specifies the priority of the queue scaling policy.
        The valid value ranges from `1` to `100`. The larger value means the higher priority.
        """
        return pulumi.get(self, "priority")

    @priority.setter
    def priority(self, value: pulumi.Input[int]):
        pulumi.set(self, "priority", value)


@pulumi.input_type
class QueueSparkDriverArgs:
    def __init__(__self__, *,
                 max_concurrent: Optional[pulumi.Input[int]] = None,
                 max_instance: Optional[pulumi.Input[int]] = None,
                 max_prefetch_instance: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[int] max_concurrent: Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
               The valid value ranges from `1` to `32`.
        :param pulumi.Input[int] max_instance: Specifies the maximum number of spark drivers that can be started on the queue.
               If the `cu_count` is `16`, the value can only be `2`.
               If The `cu_count` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
               divided by `16`.
        :param pulumi.Input[str] max_prefetch_instance: Specifies the maximum number of spark drivers to be pre-started on the queue.
               The minimum value is `0`. If the `cu_count` is less than `32`, the maximum value is `1`.
               If the `cu_count` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.
        """
        if max_concurrent is not None:
            pulumi.set(__self__, "max_concurrent", max_concurrent)
        if max_instance is not None:
            pulumi.set(__self__, "max_instance", max_instance)
        if max_prefetch_instance is not None:
            pulumi.set(__self__, "max_prefetch_instance", max_prefetch_instance)

    @property
    @pulumi.getter(name="maxConcurrent")
    def max_concurrent(self) -> Optional[pulumi.Input[int]]:
        """
        Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
        The valid value ranges from `1` to `32`.
        """
        return pulumi.get(self, "max_concurrent")

    @max_concurrent.setter
    def max_concurrent(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_concurrent", value)

    @property
    @pulumi.getter(name="maxInstance")
    def max_instance(self) -> Optional[pulumi.Input[int]]:
        """
        Specifies the maximum number of spark drivers that can be started on the queue.
        If the `cu_count` is `16`, the value can only be `2`.
        If The `cu_count` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
        divided by `16`.
        """
        return pulumi.get(self, "max_instance")

    @max_instance.setter
    def max_instance(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_instance", value)

    @property
    @pulumi.getter(name="maxPrefetchInstance")
    def max_prefetch_instance(self) -> Optional[pulumi.Input[str]]:
        """
        Specifies the maximum number of spark drivers to be pre-started on the queue.
        The minimum value is `0`. If the `cu_count` is less than `32`, the maximum value is `1`.
        If the `cu_count` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.
        """
        return pulumi.get(self, "max_prefetch_instance")

    @max_prefetch_instance.setter
    def max_prefetch_instance(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "max_prefetch_instance", value)


@pulumi.input_type
class SparkJobDependentPackageArgs:
    def __init__(__self__, *,
                 group_name: pulumi.Input[str],
                 packages: pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]]):
        """
        :param pulumi.Input[str] group_name: Specifies the user group name.  
               Only letters, digits, dots (.), hyphens (-) and underscores (_) are allowed.
               Changing this parameter will submit a new spark job.
        :param pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]] packages: Specifies the user group resource for details.
               Changing this parameter will submit a new spark job.
               The object structure is documented below.
        """
        pulumi.set(__self__, "group_name", group_name)
        pulumi.set(__self__, "packages", packages)

    @property
    @pulumi.getter(name="groupName")
    def group_name(self) -> pulumi.Input[str]:
        """
        Specifies the user group name.  
        Only letters, digits, dots (.), hyphens (-) and underscores (_) are allowed.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "group_name")

    @group_name.setter
    def group_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "group_name", value)

    @property
    @pulumi.getter
    def packages(self) -> pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]]:
        """
        Specifies the user group resource for details.
        Changing this parameter will submit a new spark job.
        The object structure is documented below.
        """
        return pulumi.get(self, "packages")

    @packages.setter
    def packages(self, value: pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]]):
        pulumi.set(self, "packages", value)


@pulumi.input_type
class SparkJobDependentPackagePackageArgs:
    def __init__(__self__, *,
                 package_name: pulumi.Input[str],
                 type: pulumi.Input[str]):
        """
        :param pulumi.Input[str] package_name: Specifies the resource name of the package.
               Changing this parameter will submit a new spark job.
        :param pulumi.Input[str] type: Specifies the resource type of the package.
               Changing this parameter will submit a new spark job.
        """
        pulumi.set(__self__, "package_name", package_name)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter(name="packageName")
    def package_name(self) -> pulumi.Input[str]:
        """
        Specifies the resource name of the package.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "package_name")

    @package_name.setter
    def package_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "package_name", value)

    @property
    @pulumi.getter
    def type(self) -> pulumi.Input[str]:
        """
        Specifies the resource type of the package.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: pulumi.Input[str]):
        pulumi.set(self, "type", value)


@pulumi.input_type
class SqlJobConfArgs:
    def __init__(__self__, *,
                 dli_sql_job_timeout: Optional[pulumi.Input[int]] = None,
                 dli_sql_sqlasync_enabled: Optional[pulumi.Input[bool]] = None,
                 spark_sql_auto_broadcast_join_threshold: Optional[pulumi.Input[int]] = None,
                 spark_sql_bad_records_path: Optional[pulumi.Input[str]] = None,
                 spark_sql_dynamic_partition_overwrite_enabled: Optional[pulumi.Input[bool]] = None,
                 spark_sql_files_max_partition_bytes: Optional[pulumi.Input[int]] = None,
                 spark_sql_max_records_per_file: Optional[pulumi.Input[int]] = None,
                 spark_sql_shuffle_partitions: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] dli_sql_job_timeout: Sets the job running timeout interval. If the timeout interval
               expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        :param pulumi.Input[bool] dli_sql_sqlasync_enabled: Specifies whether DDL and DCL statements are executed
               asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
               Changing this parameter will create a new resource.
        :param pulumi.Input[int] spark_sql_auto_broadcast_join_threshold: Maximum size of the table that
               displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
               Default value is `209715200`. Changing this parameter will create a new resource.
        :param pulumi.Input[str] spark_sql_bad_records_path: Path of bad records. Changing this parameter will create
               a new resource.
        :param pulumi.Input[bool] spark_sql_dynamic_partition_overwrite_enabled: In dynamic mode, Spark does not delete
               the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
               Changing this parameter will create a new resource.
        :param pulumi.Input[int] spark_sql_files_max_partition_bytes: Maximum number of bytes to be packed into a
               single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
               resource.
        :param pulumi.Input[int] spark_sql_max_records_per_file: Maximum number of records to be written
               into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
               Changing this parameter will create a new resource.
        :param pulumi.Input[int] spark_sql_shuffle_partitions: Default number of partitions used to filter
               data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
        if dli_sql_job_timeout is not None:
            pulumi.set(__self__, "dli_sql_job_timeout", dli_sql_job_timeout)
        if dli_sql_sqlasync_enabled is not None:
            pulumi.set(__self__, "dli_sql_sqlasync_enabled", dli_sql_sqlasync_enabled)
        if spark_sql_auto_broadcast_join_threshold is not None:
            pulumi.set(__self__, "spark_sql_auto_broadcast_join_threshold", spark_sql_auto_broadcast_join_threshold)
        if spark_sql_bad_records_path is not None:
            pulumi.set(__self__, "spark_sql_bad_records_path", spark_sql_bad_records_path)
        if spark_sql_dynamic_partition_overwrite_enabled is not None:
            pulumi.set(__self__, "spark_sql_dynamic_partition_overwrite_enabled", spark_sql_dynamic_partition_overwrite_enabled)
        if spark_sql_files_max_partition_bytes is not None:
            pulumi.set(__self__, "spark_sql_files_max_partition_bytes", spark_sql_files_max_partition_bytes)
        if spark_sql_max_records_per_file is not None:
            pulumi.set(__self__, "spark_sql_max_records_per_file", spark_sql_max_records_per_file)
        if spark_sql_shuffle_partitions is not None:
            pulumi.set(__self__, "spark_sql_shuffle_partitions", spark_sql_shuffle_partitions)

    @property
    @pulumi.getter(name="dliSqlJobTimeout")
    def dli_sql_job_timeout(self) -> Optional[pulumi.Input[int]]:
        """
        Sets the job running timeout interval. If the timeout interval
        expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "dli_sql_job_timeout")

    @dli_sql_job_timeout.setter
    def dli_sql_job_timeout(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "dli_sql_job_timeout", value)

    @property
    @pulumi.getter(name="dliSqlSqlasyncEnabled")
    def dli_sql_sqlasync_enabled(self) -> Optional[pulumi.Input[bool]]:
        """
        Specifies whether DDL and DCL statements are executed
        asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "dli_sql_sqlasync_enabled")

    @dli_sql_sqlasync_enabled.setter
    def dli_sql_sqlasync_enabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "dli_sql_sqlasync_enabled", value)

    @property
    @pulumi.getter(name="sparkSqlAutoBroadcastJoinThreshold")
    def spark_sql_auto_broadcast_join_threshold(self) -> Optional[pulumi.Input[int]]:
        """
        Maximum size of the table that
        displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
        Default value is `209715200`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_auto_broadcast_join_threshold")

    @spark_sql_auto_broadcast_join_threshold.setter
    def spark_sql_auto_broadcast_join_threshold(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "spark_sql_auto_broadcast_join_threshold", value)

    @property
    @pulumi.getter(name="sparkSqlBadRecordsPath")
    def spark_sql_bad_records_path(self) -> Optional[pulumi.Input[str]]:
        """
        Path of bad records. Changing this parameter will create
        a new resource.
        """
        return pulumi.get(self, "spark_sql_bad_records_path")

    @spark_sql_bad_records_path.setter
    def spark_sql_bad_records_path(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "spark_sql_bad_records_path", value)

    @property
    @pulumi.getter(name="sparkSqlDynamicPartitionOverwriteEnabled")
    def spark_sql_dynamic_partition_overwrite_enabled(self) -> Optional[pulumi.Input[bool]]:
        """
        In dynamic mode, Spark does not delete
        the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_dynamic_partition_overwrite_enabled")

    @spark_sql_dynamic_partition_overwrite_enabled.setter
    def spark_sql_dynamic_partition_overwrite_enabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "spark_sql_dynamic_partition_overwrite_enabled", value)

    @property
    @pulumi.getter(name="sparkSqlFilesMaxPartitionBytes")
    def spark_sql_files_max_partition_bytes(self) -> Optional[pulumi.Input[int]]:
        """
        Maximum number of bytes to be packed into a
        single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "spark_sql_files_max_partition_bytes")

    @spark_sql_files_max_partition_bytes.setter
    def spark_sql_files_max_partition_bytes(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "spark_sql_files_max_partition_bytes", value)

    @property
    @pulumi.getter(name="sparkSqlMaxRecordsPerFile")
    def spark_sql_max_records_per_file(self) -> Optional[pulumi.Input[int]]:
        """
        Maximum number of records to be written
        into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_max_records_per_file")

    @spark_sql_max_records_per_file.setter
    def spark_sql_max_records_per_file(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "spark_sql_max_records_per_file", value)

    @property
    @pulumi.getter(name="sparkSqlShufflePartitions")
    def spark_sql_shuffle_partitions(self) -> Optional[pulumi.Input[int]]:
        """
        Default number of partitions used to filter
        data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_shuffle_partitions")

    @spark_sql_shuffle_partitions.setter
    def spark_sql_shuffle_partitions(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "spark_sql_shuffle_partitions", value)


@pulumi.input_type
class TableColumnArgs:
    def __init__(__self__, *,
                 name: pulumi.Input[str],
                 type: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None,
                 is_partition: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[str] name: Specifies the name of column. Changing this parameter will create a new
               resource.
        :param pulumi.Input[str] type: Specifies data type of column. Changing this parameter will create a new
               resource.
        :param pulumi.Input[str] description: Specifies the description of column. Changing this parameter will
               create a new resource.
        :param pulumi.Input[bool] is_partition: Specifies whether the column is a partition column. The value
               `true` indicates a partition column, and the value false indicates a non-partition column. The default value
               is false. Changing this parameter will create a new resource.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "type", type)
        if description is not None:
            pulumi.set(__self__, "description", description)
        if is_partition is not None:
            pulumi.set(__self__, "is_partition", is_partition)

    @property
    @pulumi.getter
    def name(self) -> pulumi.Input[str]:
        """
        Specifies the name of column. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: pulumi.Input[str]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter
    def type(self) -> pulumi.Input[str]:
        """
        Specifies data type of column. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: pulumi.Input[str]):
        pulumi.set(self, "type", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        """
        Specifies the description of column. Changing this parameter will
        create a new resource.
        """
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)

    @property
    @pulumi.getter(name="isPartition")
    def is_partition(self) -> Optional[pulumi.Input[bool]]:
        """
        Specifies whether the column is a partition column. The value
        `true` indicates a partition column, and the value false indicates a non-partition column. The default value
        is false. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "is_partition")

    @is_partition.setter
    def is_partition(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "is_partition", value)


