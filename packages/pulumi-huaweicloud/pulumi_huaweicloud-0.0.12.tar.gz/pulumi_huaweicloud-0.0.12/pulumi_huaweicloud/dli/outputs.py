# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
from .. import _utilities
from . import outputs

__all__ = [
    'QueueScalingPolicy',
    'QueueSparkDriver',
    'SparkJobDependentPackage',
    'SparkJobDependentPackagePackage',
    'SqlJobConf',
    'TableColumn',
]

@pulumi.output_type
class QueueScalingPolicy(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "impactStartTime":
            suggest = "impact_start_time"
        elif key == "impactStopTime":
            suggest = "impact_stop_time"
        elif key == "maxCu":
            suggest = "max_cu"
        elif key == "minCu":
            suggest = "min_cu"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in QueueScalingPolicy. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        QueueScalingPolicy.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        QueueScalingPolicy.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 impact_start_time: str,
                 impact_stop_time: str,
                 max_cu: int,
                 min_cu: int,
                 priority: int):
        """
        :param str impact_start_time: Specifies the effective time of the queue scaling policy.
               The value can be set only by hour.
        :param str impact_stop_time: Specifies the expiration time of the queue scaling policy.
               The value can be set only by hour.
        :param int max_cu: Specifies the maximum number of CUs allowed by the scaling policy.
               The number must be a multiple of `4`.
        :param int min_cu: Specifies the minimum number of CUs allowed by the scaling policy.
               The number must be a multiple of `4`.
        :param int priority: Specifies the priority of the queue scaling policy.
               The valid value ranges from `1` to `100`. The larger value means the higher priority.
        """
        pulumi.set(__self__, "impact_start_time", impact_start_time)
        pulumi.set(__self__, "impact_stop_time", impact_stop_time)
        pulumi.set(__self__, "max_cu", max_cu)
        pulumi.set(__self__, "min_cu", min_cu)
        pulumi.set(__self__, "priority", priority)

    @property
    @pulumi.getter(name="impactStartTime")
    def impact_start_time(self) -> str:
        """
        Specifies the effective time of the queue scaling policy.
        The value can be set only by hour.
        """
        return pulumi.get(self, "impact_start_time")

    @property
    @pulumi.getter(name="impactStopTime")
    def impact_stop_time(self) -> str:
        """
        Specifies the expiration time of the queue scaling policy.
        The value can be set only by hour.
        """
        return pulumi.get(self, "impact_stop_time")

    @property
    @pulumi.getter(name="maxCu")
    def max_cu(self) -> int:
        """
        Specifies the maximum number of CUs allowed by the scaling policy.
        The number must be a multiple of `4`.
        """
        return pulumi.get(self, "max_cu")

    @property
    @pulumi.getter(name="minCu")
    def min_cu(self) -> int:
        """
        Specifies the minimum number of CUs allowed by the scaling policy.
        The number must be a multiple of `4`.
        """
        return pulumi.get(self, "min_cu")

    @property
    @pulumi.getter
    def priority(self) -> int:
        """
        Specifies the priority of the queue scaling policy.
        The valid value ranges from `1` to `100`. The larger value means the higher priority.
        """
        return pulumi.get(self, "priority")


@pulumi.output_type
class QueueSparkDriver(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxConcurrent":
            suggest = "max_concurrent"
        elif key == "maxInstance":
            suggest = "max_instance"
        elif key == "maxPrefetchInstance":
            suggest = "max_prefetch_instance"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in QueueSparkDriver. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        QueueSparkDriver.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        QueueSparkDriver.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_concurrent: Optional[int] = None,
                 max_instance: Optional[int] = None,
                 max_prefetch_instance: Optional[str] = None):
        """
        :param int max_concurrent: Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
               The valid value ranges from `1` to `32`.
        :param int max_instance: Specifies the maximum number of spark drivers that can be started on the queue.
               If the `cu_count` is `16`, the value can only be `2`.
               If The `cu_count` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
               divided by `16`.
        :param str max_prefetch_instance: Specifies the maximum number of spark drivers to be pre-started on the queue.
               The minimum value is `0`. If the `cu_count` is less than `32`, the maximum value is `1`.
               If the `cu_count` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.
        """
        if max_concurrent is not None:
            pulumi.set(__self__, "max_concurrent", max_concurrent)
        if max_instance is not None:
            pulumi.set(__self__, "max_instance", max_instance)
        if max_prefetch_instance is not None:
            pulumi.set(__self__, "max_prefetch_instance", max_prefetch_instance)

    @property
    @pulumi.getter(name="maxConcurrent")
    def max_concurrent(self) -> Optional[int]:
        """
        Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
        The valid value ranges from `1` to `32`.
        """
        return pulumi.get(self, "max_concurrent")

    @property
    @pulumi.getter(name="maxInstance")
    def max_instance(self) -> Optional[int]:
        """
        Specifies the maximum number of spark drivers that can be started on the queue.
        If the `cu_count` is `16`, the value can only be `2`.
        If The `cu_count` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
        divided by `16`.
        """
        return pulumi.get(self, "max_instance")

    @property
    @pulumi.getter(name="maxPrefetchInstance")
    def max_prefetch_instance(self) -> Optional[str]:
        """
        Specifies the maximum number of spark drivers to be pre-started on the queue.
        The minimum value is `0`. If the `cu_count` is less than `32`, the maximum value is `1`.
        If the `cu_count` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.
        """
        return pulumi.get(self, "max_prefetch_instance")


@pulumi.output_type
class SparkJobDependentPackage(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "groupName":
            suggest = "group_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in SparkJobDependentPackage. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        SparkJobDependentPackage.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        SparkJobDependentPackage.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 group_name: str,
                 packages: Sequence['outputs.SparkJobDependentPackagePackage']):
        """
        :param str group_name: Specifies the user group name.  
               Only letters, digits, dots (.), hyphens (-) and underscores (_) are allowed.
               Changing this parameter will submit a new spark job.
        :param Sequence['SparkJobDependentPackagePackageArgs'] packages: Specifies the user group resource for details.
               Changing this parameter will submit a new spark job.
               The object structure is documented below.
        """
        pulumi.set(__self__, "group_name", group_name)
        pulumi.set(__self__, "packages", packages)

    @property
    @pulumi.getter(name="groupName")
    def group_name(self) -> str:
        """
        Specifies the user group name.  
        Only letters, digits, dots (.), hyphens (-) and underscores (_) are allowed.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "group_name")

    @property
    @pulumi.getter
    def packages(self) -> Sequence['outputs.SparkJobDependentPackagePackage']:
        """
        Specifies the user group resource for details.
        Changing this parameter will submit a new spark job.
        The object structure is documented below.
        """
        return pulumi.get(self, "packages")


@pulumi.output_type
class SparkJobDependentPackagePackage(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "packageName":
            suggest = "package_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in SparkJobDependentPackagePackage. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        SparkJobDependentPackagePackage.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        SparkJobDependentPackagePackage.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 package_name: str,
                 type: str):
        """
        :param str package_name: Specifies the resource name of the package.
               Changing this parameter will submit a new spark job.
        :param str type: Specifies the resource type of the package.
               Changing this parameter will submit a new spark job.
        """
        pulumi.set(__self__, "package_name", package_name)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter(name="packageName")
    def package_name(self) -> str:
        """
        Specifies the resource name of the package.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "package_name")

    @property
    @pulumi.getter
    def type(self) -> str:
        """
        Specifies the resource type of the package.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class SqlJobConf(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "dliSqlJobTimeout":
            suggest = "dli_sql_job_timeout"
        elif key == "dliSqlSqlasyncEnabled":
            suggest = "dli_sql_sqlasync_enabled"
        elif key == "sparkSqlAutoBroadcastJoinThreshold":
            suggest = "spark_sql_auto_broadcast_join_threshold"
        elif key == "sparkSqlBadRecordsPath":
            suggest = "spark_sql_bad_records_path"
        elif key == "sparkSqlDynamicPartitionOverwriteEnabled":
            suggest = "spark_sql_dynamic_partition_overwrite_enabled"
        elif key == "sparkSqlFilesMaxPartitionBytes":
            suggest = "spark_sql_files_max_partition_bytes"
        elif key == "sparkSqlMaxRecordsPerFile":
            suggest = "spark_sql_max_records_per_file"
        elif key == "sparkSqlShufflePartitions":
            suggest = "spark_sql_shuffle_partitions"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in SqlJobConf. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        SqlJobConf.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        SqlJobConf.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dli_sql_job_timeout: Optional[int] = None,
                 dli_sql_sqlasync_enabled: Optional[bool] = None,
                 spark_sql_auto_broadcast_join_threshold: Optional[int] = None,
                 spark_sql_bad_records_path: Optional[str] = None,
                 spark_sql_dynamic_partition_overwrite_enabled: Optional[bool] = None,
                 spark_sql_files_max_partition_bytes: Optional[int] = None,
                 spark_sql_max_records_per_file: Optional[int] = None,
                 spark_sql_shuffle_partitions: Optional[int] = None):
        """
        :param int dli_sql_job_timeout: Sets the job running timeout interval. If the timeout interval
               expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        :param bool dli_sql_sqlasync_enabled: Specifies whether DDL and DCL statements are executed
               asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
               Changing this parameter will create a new resource.
        :param int spark_sql_auto_broadcast_join_threshold: Maximum size of the table that
               displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
               Default value is `209715200`. Changing this parameter will create a new resource.
        :param str spark_sql_bad_records_path: Path of bad records. Changing this parameter will create
               a new resource.
        :param bool spark_sql_dynamic_partition_overwrite_enabled: In dynamic mode, Spark does not delete
               the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
               Changing this parameter will create a new resource.
        :param int spark_sql_files_max_partition_bytes: Maximum number of bytes to be packed into a
               single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
               resource.
        :param int spark_sql_max_records_per_file: Maximum number of records to be written
               into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
               Changing this parameter will create a new resource.
        :param int spark_sql_shuffle_partitions: Default number of partitions used to filter
               data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
        if dli_sql_job_timeout is not None:
            pulumi.set(__self__, "dli_sql_job_timeout", dli_sql_job_timeout)
        if dli_sql_sqlasync_enabled is not None:
            pulumi.set(__self__, "dli_sql_sqlasync_enabled", dli_sql_sqlasync_enabled)
        if spark_sql_auto_broadcast_join_threshold is not None:
            pulumi.set(__self__, "spark_sql_auto_broadcast_join_threshold", spark_sql_auto_broadcast_join_threshold)
        if spark_sql_bad_records_path is not None:
            pulumi.set(__self__, "spark_sql_bad_records_path", spark_sql_bad_records_path)
        if spark_sql_dynamic_partition_overwrite_enabled is not None:
            pulumi.set(__self__, "spark_sql_dynamic_partition_overwrite_enabled", spark_sql_dynamic_partition_overwrite_enabled)
        if spark_sql_files_max_partition_bytes is not None:
            pulumi.set(__self__, "spark_sql_files_max_partition_bytes", spark_sql_files_max_partition_bytes)
        if spark_sql_max_records_per_file is not None:
            pulumi.set(__self__, "spark_sql_max_records_per_file", spark_sql_max_records_per_file)
        if spark_sql_shuffle_partitions is not None:
            pulumi.set(__self__, "spark_sql_shuffle_partitions", spark_sql_shuffle_partitions)

    @property
    @pulumi.getter(name="dliSqlJobTimeout")
    def dli_sql_job_timeout(self) -> Optional[int]:
        """
        Sets the job running timeout interval. If the timeout interval
        expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "dli_sql_job_timeout")

    @property
    @pulumi.getter(name="dliSqlSqlasyncEnabled")
    def dli_sql_sqlasync_enabled(self) -> Optional[bool]:
        """
        Specifies whether DDL and DCL statements are executed
        asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "dli_sql_sqlasync_enabled")

    @property
    @pulumi.getter(name="sparkSqlAutoBroadcastJoinThreshold")
    def spark_sql_auto_broadcast_join_threshold(self) -> Optional[int]:
        """
        Maximum size of the table that
        displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
        Default value is `209715200`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_auto_broadcast_join_threshold")

    @property
    @pulumi.getter(name="sparkSqlBadRecordsPath")
    def spark_sql_bad_records_path(self) -> Optional[str]:
        """
        Path of bad records. Changing this parameter will create
        a new resource.
        """
        return pulumi.get(self, "spark_sql_bad_records_path")

    @property
    @pulumi.getter(name="sparkSqlDynamicPartitionOverwriteEnabled")
    def spark_sql_dynamic_partition_overwrite_enabled(self) -> Optional[bool]:
        """
        In dynamic mode, Spark does not delete
        the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_dynamic_partition_overwrite_enabled")

    @property
    @pulumi.getter(name="sparkSqlFilesMaxPartitionBytes")
    def spark_sql_files_max_partition_bytes(self) -> Optional[int]:
        """
        Maximum number of bytes to be packed into a
        single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "spark_sql_files_max_partition_bytes")

    @property
    @pulumi.getter(name="sparkSqlMaxRecordsPerFile")
    def spark_sql_max_records_per_file(self) -> Optional[int]:
        """
        Maximum number of records to be written
        into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_max_records_per_file")

    @property
    @pulumi.getter(name="sparkSqlShufflePartitions")
    def spark_sql_shuffle_partitions(self) -> Optional[int]:
        """
        Default number of partitions used to filter
        data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_shuffle_partitions")


@pulumi.output_type
class TableColumn(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "isPartition":
            suggest = "is_partition"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableColumn. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableColumn.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableColumn.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 name: str,
                 type: str,
                 description: Optional[str] = None,
                 is_partition: Optional[bool] = None):
        """
        :param str name: Specifies the name of column. Changing this parameter will create a new
               resource.
        :param str type: Specifies data type of column. Changing this parameter will create a new
               resource.
        :param str description: Specifies the description of column. Changing this parameter will
               create a new resource.
        :param bool is_partition: Specifies whether the column is a partition column. The value
               `true` indicates a partition column, and the value false indicates a non-partition column. The default value
               is false. Changing this parameter will create a new resource.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "type", type)
        if description is not None:
            pulumi.set(__self__, "description", description)
        if is_partition is not None:
            pulumi.set(__self__, "is_partition", is_partition)

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        Specifies the name of column. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def type(self) -> str:
        """
        Specifies data type of column. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "type")

    @property
    @pulumi.getter
    def description(self) -> Optional[str]:
        """
        Specifies the description of column. Changing this parameter will
        create a new resource.
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter(name="isPartition")
    def is_partition(self) -> Optional[bool]:
        """
        Specifies whether the column is a partition column. The value
        `true` indicates a partition column, and the value false indicates a non-partition column. The default value
        is false. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "is_partition")


